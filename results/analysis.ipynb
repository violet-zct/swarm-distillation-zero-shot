{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ed7569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis.ipynb\n",
      "anli_none_dev_r1_bigscience.T0\n",
      "anli_none_dev_r1_bigscience.T0_3B\n",
      "anli_none_dev_r2_bigscience.T0\n",
      "anli_none_dev_r2_bigscience.T0_3B\n",
      "anli_none_dev_r3_bigscience.T0\n",
      "anli_none_dev_r3_bigscience.T0_3B\n",
      "hellaswag_none_validation_bigscience.T0\n",
      "hellaswag_none_validation_bigscience.T0_3B\n",
      "super_glue_cb_validation_bigscience.T0\n",
      "super_glue_cb_validation_bigscience.T0_3B\n",
      "super_glue_copa_validation_bigscience.T0\n",
      "super_glue_copa_validation_bigscience.T0_3B\n",
      "super_glue_rte_validation_bigscience.T0\n",
      "super_glue_rte_validation_bigscience.T0_3B\n",
      "super_glue_wic_validation_bigscience.T0\n",
      "super_glue_wic_validation_bigscience.T0_3B\n",
      "super_glue_wsc.fixed_validation_bigscience.T0\n",
      "super_glue_wsc.fixed_validation_bigscience.T0_3B\n",
      "winogrande_winogrande_xl_validation_bigscience.T0\n",
      "winogrande_winogrande_xl_validation_bigscience.T0_3B\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac25ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset anli (/data/home/chuntinz/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01484537124633789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 9,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ba6787826941439b149070e00c3c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-3 style', 'MNLI crowdsource', 'always/sometimes/never', 'based on the previous passage', 'can we infer', 'claim true/false/inconclusive', 'consider always/sometimes/never', 'does it follow that', 'does this imply', 'guaranteed true', 'guaranteed/possible/impossible', 'justified in saying', 'must be true', 'should assume', 'take the following as truth']\n",
      "Guaranteed\n",
      "========\n",
      "{'uid': 'e66af435-eef1-491b-af2a-4825d54611e1', 'premise': 'Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right \"Forza Italia\" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci.', 'hypothesis': 'Franco Zeffirelli had a political career', 'label': 0, 'reason': 'Franco Zeffirelli was a senator so he had a political career. The system likely was fooled because I used words not used in the context.'}\n",
      "===========\n",
      "Assume it is true that Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right \"Forza Italia\" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. \n",
      "\n",
      "Therefore, \"Franco Zeffirelli had a political career\" is guaranteed, possible, or impossible?\n",
      "========\n",
      "Guaranteed\n",
      "\n",
      "['Guaranteed', 'Possible', 'Impossible']\n"
     ]
    }
   ],
   "source": [
    "from promptsource.templates import DatasetTemplates\n",
    "import datasets\n",
    "data = datasets.load_dataset(\"anli\")\n",
    "data = data[\"dev_r1\"]\n",
    "prompts = DatasetTemplates(\"anli\", None)\n",
    "prompt_names = prompts.all_template_names\n",
    "print(prompt_names)\n",
    "pname = 'guaranteed/possible/impossible'\n",
    "k = 1\n",
    "input, output = prompts[pname].apply(data[k])\n",
    "targets = prompts[pname].get_answer_choices_list(data[k])\n",
    "\n",
    "print(output.strip())\n",
    "print(\"========\")\n",
    "print(data[k])\n",
    "print(\"===========\")\n",
    "print(input)\n",
    "print(\"========\")\n",
    "print(output)\n",
    "print()\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e83225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10}\n",
      "anli_none_dev_r1_bigscience.T0: max_acc = 46.5, oracle_acc = 80.2, median_acc = 44.7, min_acc = 33.3, ensemble_acc = 45.5\n",
      "{10}\n",
      "anli_none_dev_r2_bigscience.T0: max_acc = 40.8, oracle_acc = 78.9, median_acc = 39.7, min_acc = 33.5, ensemble_acc = 39.9\n",
      "{10}\n",
      "anli_none_dev_r3_bigscience.T0: max_acc = 44.25, oracle_acc = 79.58333333333333, median_acc = 42.67, min_acc = 33.92, ensemble_acc = 43.33\n",
      "set()\n",
      "hellaswag_none_validation_bigscience.T0: max_acc = 34.13, oracle_acc = 64.35968930491934, median_acc = 33.67, min_acc = 33.18, ensemble_acc = 34.8\n",
      "{10}\n",
      "super_glue_cb_validation_bigscience.T0: max_acc = 80.36, oracle_acc = 98.21428571428571, median_acc = 78.57, min_acc = 8.93, ensemble_acc = 78.57\n",
      "set()\n",
      "super_glue_copa_validation_bigscience.T0: max_acc = 96.0, oracle_acc = 99.0, median_acc = 93.0, min_acc = 82.0, ensemble_acc = 94.0\n",
      "set()\n",
      "super_glue_rte_validation_bigscience.T0: max_acc = 84.84, oracle_acc = 93.14079422382672, median_acc = 82.31, min_acc = 73.65, ensemble_acc = 84.48\n",
      "{1, 3, 5, 7}\n",
      "super_glue_wic_validation_bigscience.T0: max_acc = 58.46, oracle_acc = 89.96865203761756, median_acc = 56.66, min_acc = 51.72, ensemble_acc = 55.96\n",
      "{7}\n",
      "super_glue_wsc.fixed_validation_bigscience.T0: max_acc = 69.23, oracle_acc = 97.11538461538461, median_acc = 62.5, min_acc = 49.04, ensemble_acc = 63.46\n",
      "{2}\n",
      "winogrande_winogrande_xl_validation_bigscience.T0: max_acc = 63.77, oracle_acc = 79.55801104972376, median_acc = 60.38, min_acc = 56.27, ensemble_acc = 60.62\n",
      "{1, 2, 10, 6}\n",
      "anli_none_dev_r1_bigscience.T0_3B: max_acc = 36.0, oracle_acc = 52.4, median_acc = 33.6, min_acc = 32.4, ensemble_acc = 34.2\n",
      "{1, 2, 10, 6}\n",
      "anli_none_dev_r2_bigscience.T0_3B: max_acc = 35.1, oracle_acc = 53.2, median_acc = 33.4, min_acc = 30.4, ensemble_acc = 32.9\n",
      "{1, 2, 4, 6, 8, 10}\n",
      "anli_none_dev_r3_bigscience.T0_3B: max_acc = 34.08, oracle_acc = 50.0, median_acc = 33.33, min_acc = 32.58, ensemble_acc = 33.25\n",
      "set()\n",
      "hellaswag_none_validation_bigscience.T0_3B: max_acc = 28.46, oracle_acc = 60.446126269667396, median_acc = 27.32, min_acc = 25.65, ensemble_acc = 27.87\n",
      "{1, 2, 10}\n",
      "super_glue_cb_validation_bigscience.T0_3B: max_acc = 64.29, oracle_acc = 85.71428571428571, median_acc = 50.0, min_acc = 8.93, ensemble_acc = 53.57\n",
      "set()\n",
      "super_glue_copa_validation_bigscience.T0_3B: max_acc = 84.0, oracle_acc = 94.0, median_acc = 79.0, min_acc = 61.0, ensemble_acc = 81.0\n",
      "{1, 3, 4, 5, 7}\n",
      "super_glue_rte_validation_bigscience.T0_3B: max_acc = 70.04, oracle_acc = 77.25631768953069, median_acc = 64.08, min_acc = 59.57, ensemble_acc = 66.79\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
      "super_glue_wic_validation_bigscience.T0_3B: max_acc = 52.51, oracle_acc = 52.507836990595614, median_acc = 50.39, min_acc = 49.69, ensemble_acc = 50.16\n",
      "{3, 4, 5, 7, 8, 9}\n",
      "super_glue_wsc.fixed_validation_bigscience.T0_3B: max_acc = 69.23, oracle_acc = 85.57692307692308, median_acc = 64.9, min_acc = 60.58, ensemble_acc = 66.35\n",
      "set()\n",
      "winogrande_winogrande_xl_validation_bigscience.T0_3B: max_acc = 52.49, oracle_acc = 79.08445146014206, median_acc = 50.51, min_acc = 49.25, ensemble_acc = 51.07\n",
      "correlation = (0.8691869245498127, 6.852489746191231e-34)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import scipy.stats\n",
    "\n",
    "def read_file(fname):\n",
    "    lidx = -1\n",
    "    eidx = -1\n",
    "    all_preds_per_example = []\n",
    "    all_golds = []\n",
    "    all_preds_per_prompt = []\n",
    "    with open(fname) as fin:\n",
    "        for line in fin:\n",
    "            lidx += 1\n",
    "            if lidx == 0:\n",
    "                fields = line.strip().split(',')\n",
    "                num_prompts, num_examples = int(fields[0].split(\"=\")[1].strip()), int(fields[1].split(\"=\")[1].strip())\n",
    "                all_preds_per_prompt = [[] for _ in range(num_prompts)]\n",
    "            elif lidx == 1:\n",
    "                fields = line.strip().split(',')\n",
    "                max_acc, median_acc, min_acc, ensemble_acc = float(fields[0].split(\"=\")[1].strip()), float(fields[1].split(\"=\")[1].strip()), float(fields[3].split(\"=\")[1].strip()), float(fields[-1].split(\"=\")[1].strip())\n",
    "            elif lidx == 2:\n",
    "                prompts_accuracy = list(map(float, line.strip().split(':')[1].strip().split()))\n",
    "            elif lidx == 3:\n",
    "                pass\n",
    "            elif line.startswith(\"=====\"):\n",
    "                eidx += 1\n",
    "            elif line.startswith(\"eid\"):\n",
    "                fields = line.strip().split(',')\n",
    "                gold = int(fields[1].split(\"=\")[1])\n",
    "                preds = list(map(int, fields[-1].strip().split()))\n",
    "                all_preds_per_example.append(preds)\n",
    "                all_golds.append(gold)\n",
    "                for pid, pp in enumerate(preds):\n",
    "                    all_preds_per_prompt[pid].append(pp)\n",
    "            else:\n",
    "                # read pred probs\n",
    "                pass\n",
    "    return num_prompts, num_examples, all_preds_per_example, all_preds_per_prompt, all_golds, prompts_accuracy, max_acc, median_acc, min_acc, ensemble_acc\n",
    "\n",
    "test_file_names = ['anli_none_dev_r1_bigscience.T0', 'anli_none_dev_r2_bigscience.T0', 'anli_none_dev_r3_bigscience.T0', \n",
    "                  'hellaswag_none_validation_bigscience.T0', 'super_glue_cb_validation_bigscience.T0',\n",
    "                  'super_glue_copa_validation_bigscience.T0', 'super_glue_rte_validation_bigscience.T0',\n",
    "                  'super_glue_wic_validation_bigscience.T0', 'super_glue_wsc.fixed_validation_bigscience.T0', 'winogrande_winogrande_xl_validation_bigscience.T0']\n",
    "\n",
    "t0_11b = {}\n",
    "for fname in test_file_names:\n",
    "    num_prompts, num_examples, all_preds_per_example, all_preds_per_prompt, all_golds, prompts_accuracy, max_acc, median_acc, min_acc, ensemble_acc = read_file(fname)\n",
    "    t0_11b[fname] = {\"nprompts\": num_prompts, 'nexamples': num_examples, 'all_preds_per_example': all_preds_per_example, 'all_preds_per_prompt': all_preds_per_prompt, \"all_golds\": all_golds, \n",
    "                    \"prompts_accuracy\": prompts_accuracy, 'max_acc': max_acc, 'median_acc': median_acc, 'min_acc': min_acc, 'ensemble_acc': ensemble_acc}\n",
    "    \n",
    "t0_3b = {}\n",
    "for fname in test_file_names:\n",
    "    num_prompts, num_examples, all_preds_per_example, all_preds_per_prompt, all_golds, prompts_accuracy, max_acc, median_acc, min_acc, ensemble_acc = read_file(fname+\"_3B\")\n",
    "    t0_3b[fname] = {\"nprompts\": num_prompts, 'nexamples': num_examples, 'all_preds_per_example': all_preds_per_example, 'all_preds_per_prompt': all_preds_per_prompt, \"all_golds\": all_golds, \n",
    "                    \"prompts_accuracy\": prompts_accuracy, 'max_acc': max_acc, 'median_acc': median_acc, 'min_acc': min_acc, 'ensemble_acc': ensemble_acc}\n",
    "    \n",
    "\n",
    "def cal_task_prompt_oracle_acc(task_dict, suffix=\"\"):\n",
    "    for fname in test_file_names:\n",
    "        results = task_dict[fname]\n",
    "        degenerate_prompts = set()\n",
    "        for pid, preds in enumerate(task_dict[fname]['all_preds_per_prompt']):\n",
    "            if len(set(preds)) == 1:\n",
    "                degenerate_prompts.add(pid)\n",
    "            else:\n",
    "                count = Counter(preds)\n",
    "                if max(np.array(list(count.values()))*1.0 / sum(count.values())) > 0.8:\n",
    "                    degenerate_prompts.add(pid)\n",
    "            \n",
    "        all_preds_per_example = task_dict[fname]['all_preds_per_example']\n",
    "        all_golds = task_dict[fname]['all_golds']\n",
    "        print(degenerate_prompts)\n",
    "        count = 0\n",
    "        for pred_per_example, gold in zip(all_preds_per_example, all_golds):\n",
    "            pred_per_example = [pred_per_example[i] for i in range(len(all_preds_per_example[0])) if i not in degenerate_prompts]\n",
    "            count += 1 if gold in pred_per_example else 0\n",
    "#             if pred_per_example.count(gold) >= 2:\n",
    "#                 count += 1\n",
    "        oracle_acc = count * 100. / len(all_golds)\n",
    "        max_acc, median_acc, min_acc, ensemble_acc = task_dict[fname]['max_acc'], task_dict[fname]['median_acc'], task_dict[fname]['min_acc'], task_dict[fname]['ensemble_acc'], \n",
    "        print(\"{}{}: max_acc = {}, oracle_acc = {}, median_acc = {}, min_acc = {}, ensemble_acc = {}\".format(fname, suffix, max_acc, oracle_acc, median_acc, min_acc, ensemble_acc))\n",
    "        \n",
    "cal_task_prompt_oracle_acc(t0_11b)\n",
    "cal_task_prompt_oracle_acc(t0_3b, \"_3B\")\n",
    "\n",
    "def cal_correlation():\n",
    "    t11b_accs, t3b_accs = [], []\n",
    "    for fname in test_file_names:\n",
    "        t11b_accs.extend(t0_11b[fname][\"prompts_accuracy\"])\n",
    "        t3b_accs.extend(t0_3b[fname][\"prompts_accuracy\"])\n",
    "    corr = scipy.stats.pearsonr(t11b_accs, t3b_accs)\n",
    "    print(\"correlation = {}\".format(corr))\n",
    "\n",
    "cal_correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc236397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "glue mrpc validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (../pretrain_models/huggingface/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014409065246582031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10170b44da8456caa06811110b4a8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 7, original prompts = 5\n",
      "dict_keys(['sentence1', 'sentence2', 'label', 'idx'])\n",
      "number of choices = 2\n",
      "['Are the following two sentences \"equivalent\" or \"not equivalent\"?\\nHe said the foodservice pie business doesn \\'t fit the company \\'s long-term growth strategy .\\n\" The foodservice pie business does not fit our long-term growth strategy .', 'equivalent']\n",
      "=====================\n",
      "{% if label == 1 %}\n",
      "Paraphrase the following sentence: {{sentence1}}\n",
      "|||\n",
      "{{sentence2}}\n",
      "{% endif %}\n",
      "=====================\n",
      "[\"Paraphrase the following sentence: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\", '\" The foodservice pie business does not fit our long-term growth strategy .']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "glue qqp validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (../pretrain_models/huggingface/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013454437255859375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ec081a84294449a85c4a34d111f005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 6, original prompts = 5\n",
      "dict_keys(['question1', 'question2', 'label', 'idx'])\n",
      "number of choices = 2\n",
      "['I received the questions \"Why are African-Americans so beautiful?\" and \"Why are hispanics so beautiful?\". Are they duplicates?', 'no']\n",
      "2\n",
      "=====================\n",
      "Can an answer to \"{{question1}}\" also be used to answer \"{{question2}}\"? ||| {{ answer_choices[label] }}\n",
      "=====================\n",
      "['Can an answer to \"Why are African-Americans so beautiful?\" also be used to answer \"Why are hispanics so beautiful?\"?', 'no']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "paws labeled_final validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset paws (../pretrain_models/huggingface/paws/labeled_final/1.1.0/8d567c6472623f42bd2cc635cad06932d0f0cd2f897db56013c1180f4317d338)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013556241989135742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a3bb9961564efc8d3525924146f789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 12, original prompts = 11\n",
      "dict_keys(['id', 'sentence1', 'sentence2', 'label'])\n",
      "number of choices = 2\n",
      "['Sentence 1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia .\\nSentence 2: Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria .\\nQuestion: Does Sentence 1 paraphrase Sentence 2? Yes or No?', 'No']\n",
      "=====================\n",
      "{% if label == 1 %} \n",
      "Paraphrase the sentence: {{sentence1}} \n",
      "||| \n",
      "{{sentence2}} \n",
      "{% endif %}\n",
      "=====================\n",
      "['']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "ai2_arc ARC-Challenge validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ai2_arc (../pretrain_models/huggingface/ai2_arc/ARC-Challenge/1.0.0/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013622760772705078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360bec26f8214a65a721ea06c5f65f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ai2_arc (../pretrain_models/huggingface/ai2_arc/ARC-Easy/1.0.0/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 6, original prompts = 5\n",
      "dict_keys(['id', 'question', 'choices', 'answerKey'])\n",
      "number of choices = 4\n",
      "[\"Here's a problem to solve: Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?\\n\\nAmong the 4 following options, which is the correct answer?\\n\\n- A: Put the objects in groups.\\n \\n- B: Change the height of the ramp.\\n \\n- C: Choose different objects to roll.\\n \\n- D: Record the details of the investigation.\", 'D']\n",
      "=====================\n",
      "Pick and copy all the incorrect options for the following question:\n",
      "\n",
      "{{question}}\n",
      "\n",
      "Options:\n",
      "- {{choices[\"text\"] | join(\"\\n- \")}}|||\n",
      "{% for i in range(choices[\"label\"]|length) %}\n",
      "{% if i != choices[\"label\"].index(answerKey) %}\n",
      "- {{choices[\"text\"][i]}}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "=====================\n",
      "['Pick and copy all the incorrect options for the following question:\\n\\nJuan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?\\n\\nOptions:\\n- Put the objects in groups.\\n- Change the height of the ramp.\\n- Choose different objects to roll.\\n- Record the details of the investigation.', '- Put the objects in groups.\\n\\n\\n\\n- Change the height of the ramp.\\n\\n\\n\\n- Choose different objects to roll.']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "ai2_arc ARC-Easy validation\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013916730880737305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a426d4c63264d33aa2a2ac75788f483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 6, original prompts = 5\n",
      "dict_keys(['id', 'question', 'choices', 'answerKey'])\n",
      "number of choices = 4\n",
      "[\"Here's a problem to solve: Which technology was developed most recently?\\n\\nAmong the 4 following options, which is the correct answer?\\n\\n- A: cellular telephone\\n \\n- B: television\\n \\n- C: refrigerator\\n \\n- D: airplane\", 'A']\n",
      "=====================\n",
      "Pick and copy all the incorrect options for the following question:\n",
      "\n",
      "{{question}}\n",
      "\n",
      "Options:\n",
      "- {{choices[\"text\"] | join(\"\\n- \")}}|||\n",
      "{% for i in range(choices[\"label\"]|length) %}\n",
      "{% if i != choices[\"label\"].index(answerKey) %}\n",
      "- {{choices[\"text\"][i]}}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "=====================\n",
      "['Pick and copy all the incorrect options for the following question:\\n\\nWhich technology was developed most recently?\\n\\nOptions:\\n- cellular telephone\\n- television\\n- refrigerator\\n- airplane', '- television\\n\\n\\n\\n- refrigerator\\n\\n\\n\\n- airplane']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "kilt_tasks hotpotqa validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset kilt_tasks (../pretrain_models/huggingface/kilt_tasks/hotpotqa/1.0.0/57dc8b2431e76637e0c6ef79689ca4af61ed3a330e2e0cd62c8971465a35db3a)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01410222053527832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67881d327cca4227915f7ef8a6ae07d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 0\n",
      "dict_keys(['id', 'input', 'meta', 'output'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "=====================\n",
      "{% if output %}\n",
      "Combine facts and answer this: {{input}}\n",
      "|||\n",
      "{{output | map(attribute=\"answer\") | list | choice}}\n",
      "{% endif %}\n",
      "=====================\n",
      "['Combine facts and answer this: Were Scott Derrickson and Ed Wood of the same nationality?', 'yes']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "trivia_qa unfiltered validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset trivia_qa (../pretrain_models/huggingface/trivia_qa/unfiltered/1.2.0/e73c5e47a8704744fa9ded33504b35a6c098661813d1c2a09892eb9b9e9d59ae)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01381063461303711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b8a35b8c614e059972a3c2a637e53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 4\n",
      "dict_keys(['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[\"I've always wondered: Who was the man behind The Chipmunks?\", 'David Seville']\n",
      "=====================\n",
      "{% if answer.aliases %} \n",
      "    Guess a question that has the answer \"{{answer.aliases|choice}}\" \n",
      "    |||  \n",
      "    {{question}} \n",
      "{% endif %}\n",
      "=====================\n",
      "['Guess a question that has the answer \"David Seville\"', 'Who was the man behind The Chipmunks?']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "web_questions None test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset web_questions (../pretrain_models/huggingface/web_questions/default/1.0.0/e6742cc64f6652db0c52cb07b5414e3c001512bf5bde7aa5587353c31db1ed8b)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013726472854614258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d7d147a3f495292884d50bcf30107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 5\n",
      "dict_keys(['url', 'question', 'answers'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Give me the correct facts to answer this: what does jamaican people speak?', 'Jamaican Creole English Language']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "wiki_qa None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_qa (../pretrain_models/huggingface/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013439416885375977,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a5161f6e7141578b6a3beff959af95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 11, original prompts = 6\n",
      "dict_keys(['question_id', 'question', 'document_title', 'answer', 'label'])\n",
      "number of choices = 2\n",
      "['This is a correct answer to the following question about Tissue (biology). Yes or no?\\nAnswer: Cross section of sclerenchyma fibers in plant ground tissue\\nQuestion: How are epithelial tissues joined together?', 'No']\n",
      "=====================\n",
      "{% if label == 1 %}\n",
      "Generate a question about the topic \"{{document_title}}\" whose answer would be: {{answer}}.|||\n",
      "{{question}}?\n",
      "{% endif %}\n",
      "\n",
      "=====================\n",
      "['']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "adversarial_qa dbidaf validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (../pretrain_models/huggingface/adversarial_qa/dbidaf/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013425111770629883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36d0a0315cb4971bb02320f562615ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (../pretrain_models/huggingface/adversarial_qa/dbert/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 4\n",
      "dict_keys(['id', 'title', 'context', 'question', 'answers', 'metadata'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Given the following passage\\n\\n\"The African Great Lakes region, which Kenya is a part of, has been inhabited by humans since the Lower Paleolithic period. By the first millennium AD, the Bantu expansion had reached the area from West-Central Africa. The borders of the modern state consequently comprise the crossroads of the Niger-Congo, Nilo-Saharan and Afroasiatic areas of the continent, representing most major ethnolinguistic groups found in Africa. Bantu and Nilotic populations together constitute around 97% of the nation\\'s residents. European and Arab presence in coastal Mombasa dates to the Early Modern period; European exploration of the interior began in the 19th century. The British Empire established the East Africa Protectorate in 1895, which starting in 1920 gave way to the Kenya Colony. Kenya obtained independence in December 1963. Following a referendum in August 2010 and adoption of a new constitution, Kenya is now divided into 47 semi-autonomous counties, governed by elected governors.\",\\n\\nanswer the following question. Note that the answer is present within the text.\\n\\nQuestion: when has people evidently existed in Kenya?', 'since the Lower Paleolithic period']\n",
      "=====================\n",
      "I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"{{context}}\"? |||\n",
      "{{question}}\n",
      "=====================\n",
      "['I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"The African Great Lakes region, which Kenya is a part of, has been inhabited by humans since the Lower Paleolithic period. By the first millennium AD, the Bantu expansion had reached the area from West-Central Africa. The borders of the modern state consequently comprise the crossroads of the Niger-Congo, Nilo-Saharan and Afroasiatic areas of the continent, representing most major ethnolinguistic groups found in Africa. Bantu and Nilotic populations together constitute around 97% of the nation\\'s residents. European and Arab presence in coastal Mombasa dates to the Early Modern period; European exploration of the interior began in the 19th century. The British Empire established the East Africa Protectorate in 1895, which starting in 1920 gave way to the Kenya Colony. Kenya obtained independence in December 1963. Following a referendum in August 2010 and adoption of a new constitution, Kenya is now divided into 47 semi-autonomous counties, governed by elected governors.\"?', 'when has people evidently existed in Kenya?']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "adversarial_qa dbert validation\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013512372970581055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f485e2dec74ba2ac32b8aa22e2eba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 4\n",
      "dict_keys(['id', 'title', 'context', 'question', 'answers', 'metadata'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Given the following passage\\n\\n\"A 2000 study found that 42% of UK teachers experienced occupational stress, twice the figure for the average profession. A 2012 study found that teachers experienced double the rate of anxiety, depression, and stress than average workers.\",\\n\\nanswer the following question. Note that the answer is present within the text.\\n\\nQuestion: What is teaching not considered due to stress?', 'average profession']\n",
      "=====================\n",
      "I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"{{context}}\"? |||\n",
      "{{question}}\n",
      "=====================\n",
      "['I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"A 2000 study found that 42% of UK teachers experienced occupational stress, twice the figure for the average profession. A 2012 study found that teachers experienced double the rate of anxiety, depression, and stress than average workers.\"?', 'What is teaching not considered due to stress?']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "adversarial_qa droberta validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (../pretrain_models/huggingface/adversarial_qa/droberta/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013473033905029297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81498a04e7ce48eb963a57089a25c62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 4\n",
      "dict_keys(['id', 'title', 'context', 'question', 'answers', 'metadata'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Given the following passage\\n\\n\"Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.\",\\n\\nanswer the following question. Note that the answer is present within the text.\\n\\nQuestion: What consumer product are the rain forests cleared for?', 'soybeans']\n",
      "=====================\n",
      "I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"{{context}}\"? |||\n",
      "{{question}}\n",
      "=====================\n",
      "['I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.\"?', 'What consumer product are the rain forests cleared for?']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "duorc SelfRC validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset duorc (../pretrain_models/huggingface/duorc/SelfRC/1.0.0/7a96356b7615d573abcd03a9328292c38348547971989538a771c32089bff199)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013312578201293945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a57ab0f98ac47bf8c5947b832d80cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset duorc (../pretrain_models/huggingface/duorc/ParaphraseRC/1.0.0/7a96356b7615d573abcd03a9328292c38348547971989538a771c32089bff199)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 9, original prompts = 5\n",
      "dict_keys(['plot_id', 'plot', 'title', 'question_id', 'question', 'answers', 'no_answer'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Please answer the following question about this movie plot. If it\\'s un-answerable, please output \"No answer\".\\n\\nQuestion: Who drinks the elixir?\\nMovie plot title: The Forbidden Kingdom\\nMovie plot: South Boston teenager Jason Tripitikas is a fan of martial arts films and awakens from a dream of a battle between the Monkey King and celestial soldiers in the clouds. He visits a pawn shop in Chinatown to buy Wuxia DVDs and discovers a golden staff. On his way home, Tripitikas is harassed by some hooligans, whose leader Lupo attempts to use him to help them rob the shop-owner Hop, who is shot by Lupo. Hop tells Tripitikas to deliver the staff to its rightful owner and Tripitikas flees with the staff. He is cornered on the rooftop before being pulled off the roof by the staff.\\nWhen Tripitikas regains consciousness, he finds himself in a village in ancient China that is under attack by armored soldiers. The soldiers see his staff and attempt to seize it. He is saved by the inebriated traveling scholar Lu Yan, a supposed \"immortal,\" who remains alert and agile even when drunk. Lu tells him the story of the rivalry between the King and the Jade Warlord. The Warlord tricked the King into setting aside his magic staff, Ruyi Jingu Bang, and transformed the immortal into a stone statue, but the King cast his staff far away before the transformation. Lu ends the tale with a prophecy about a \"Seeker\" who will find the staff and free the King. Just then, they are attacked by the Warlord\\'s men again, but manage to escape with the help of Golden Sparrow, a young woman. She reveals that her family was murdered by the Warlord, against whom she has sworn revenge.\\nMeanwhile, the Warlord, upon learning about the staff, sends the witch Ni-Chang to help him retrieve it in exchange for the elixir of immortality. Tripitikas, Lu and Sparrow meet a strange man dressed in white who takes the staff away from them. Lu fights with the man (later revealed to be the Silent Monk) for the staff until the latter realizes that Tripitikas is the Seeker, and joins them in their quest to free the King. As the four travel to Five Elements Mountain, Lu and the Monk teach Tripitikas kung fu along the way. After crossing a desert, they encounter...', 'Lu']\n",
      "=====================\n",
      "{% if no_answer == false%}\n",
      "Build a movie plot around this: {{ question }} {{answers|choice}}\n",
      "|||\n",
      "{{ plot }}\n",
      "{% endif %}\n",
      "=====================\n",
      "['Build a movie plot around this: Who drinks the elixir? Lu', 'South Boston teenager Jason Tripitikas is a fan of martial arts films and awakens from a dream of a battle between the Monkey King and celestial soldiers in the clouds. He visits a pawn shop in Chinatown to buy Wuxia DVDs and discovers a golden staff. On his way home, Tripitikas is harassed by some hooligans, whose leader Lupo attempts to use him to help them rob the shop-owner Hop, who is shot by Lupo. Hop tells Tripitikas to deliver the staff to its rightful owner and Tripitikas flees with the staff. He is cornered on the rooftop before being pulled off the roof by the staff.\\nWhen Tripitikas regains consciousness, he finds himself in a village in ancient China that is under attack by armored soldiers. The soldiers see his staff and attempt to seize it. He is saved by the inebriated traveling scholar Lu Yan, a supposed \"immortal,\" who remains alert and agile even when drunk. Lu tells him the story of the rivalry between the King and the Jade Warlord. The Warlord tricked the King into setting aside his magic staff, Ruyi Jingu Bang, and transformed the immortal into a stone statue, but the King cast his staff far away before the transformation. Lu ends the tale with a prophecy about a \"Seeker\" who will find the staff and free the King. Just then, they are attacked by the Warlord\\'s men again, but manage to escape with the help of Golden Sparrow, a young woman. She reveals that her family was murdered by the Warlord, against whom she has sworn revenge.\\nMeanwhile, the Warlord, upon learning about the staff, sends the witch Ni-Chang to help him retrieve it in exchange for the elixir of immortality. Tripitikas, Lu and Sparrow meet a strange man dressed in white who takes the staff away from them. Lu fights with the man (later revealed to be the Silent Monk) for the staff until the latter realizes that Tripitikas is the Seeker, and joins them in their quest to free the King. As the four travel to Five Elements Mountain, Lu and the Monk teach Tripitikas kung fu along the way. After crossing a desert, they encounter...']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "duorc ParaphraseRC validation\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013463258743286133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa33f58c948f4c3b84fc2545dbd8b897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 9, original prompts = 5\n",
      "dict_keys(['plot_id', 'plot', 'title', 'question_id', 'question', 'answers', 'no_answer'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Please answer the following question about this movie plot. If it\\'s un-answerable, please output \"No answer\".\\n\\nQuestion: what Zerophiliacs begin to change whenever they are sexually aroused?\\nMovie plot title: Zerophilia\\nMovie plot: \"Zerophilia\" is a fictional condition that affects an unknown number of people with an extra \"Z\" chromosome. Following their first full sexual experience, zerophiliacs begin to change sex after experiencing an orgasm. Luke (Taylor Handley), a young man somewhat insecure about his masculinity, begins to exhibit zerophilia following an encounter with a woman (Kelly Le Brock). He meets Michelle (Rebecca Mozo) and experiences partial transformations when they go out together.\\nHe confides with his best friend Keenan (Dustin Seavey) about his partial transformations, who in turn contacts Dr. Sydney Catchadourian (Gina Bellman). Dr. Catchadourian persuades Luke to go through a full transformation. Luke does this by masturbating, becoming female, and subsequently calling herself \"Luca\". Luca has difficulty achieving an orgasm to change back, even with coaching from Keenan\\'s girlfriend Janine (Alison Folland). However, a visit by Michelle\\'s attractive brother, Max (Kyle Schmid), who flirts with \"Luke\\'s cousin\", enables her to get sufficiently aroused to complete the transformation back to Luke.\\nLuke is threatened by his sex transformation, his arousal by an attractive male, and the questions of sexual identity it raises; he seeks help from Sydney. She tells him that a zerophiliac can become \"a-morphic\" and stop changing sex only by having sex with another zerophiliac... such as herself. He reluctantly agrees to do it, but discovers afterward that she was not telling him the full truth: an a-morphic zerophiliac can still change by having sex with another zerophiliac, and Dr. Catchadourian was using Luke to change herself one last time (into a man), leaving Luca as a woman in the process.\\nComic tensions arise from Luke\\'s efforts to keep Michelle at a distance, Max\\'s defensiveness about his sister, Luca\\'s half-hearted resistance to Max\\'s affections, and Luke\\'s confused aggression toward Max. When Michelle discovers that Luke had sex with Dr. Catchadourian, she feels betrayed. Hoping to find Michelle, Luca seeks out Max...', 'Gender']\n",
      "=====================\n",
      "{% if no_answer == false%}\n",
      "Build a movie plot around this: {{ question }} {{answers|choice}}\n",
      "|||\n",
      "{{ plot }}\n",
      "{% endif %}\n",
      "=====================\n",
      "['Build a movie plot around this: what Zerophiliacs begin to change whenever they are sexually aroused? Gender', '\"Zerophilia\" is a fictional condition that affects an unknown number of people with an extra \"Z\" chromosome. Following their first full sexual experience, zerophiliacs begin to change sex after experiencing an orgasm. Luke (Taylor Handley), a young man somewhat insecure about his masculinity, begins to exhibit zerophilia following an encounter with a woman (Kelly Le Brock). He meets Michelle (Rebecca Mozo) and experiences partial transformations when they go out together.\\nHe confides with his best friend Keenan (Dustin Seavey) about his partial transformations, who in turn contacts Dr. Sydney Catchadourian (Gina Bellman). Dr. Catchadourian persuades Luke to go through a full transformation. Luke does this by masturbating, becoming female, and subsequently calling herself \"Luca\". Luca has difficulty achieving an orgasm to change back, even with coaching from Keenan\\'s girlfriend Janine (Alison Folland). However, a visit by Michelle\\'s attractive brother, Max (Kyle Schmid), who flirts with \"Luke\\'s cousin\", enables her to get sufficiently aroused to complete the transformation back to Luke.\\nLuke is threatened by his sex transformation, his arousal by an attractive male, and the questions of sexual identity it raises; he seeks help from Sydney. She tells him that a zerophiliac can become \"a-morphic\" and stop changing sex only by having sex with another zerophiliac... such as herself. He reluctantly agrees to do it, but discovers afterward that she was not telling him the full truth: an a-morphic zerophiliac can still change by having sex with another zerophiliac, and Dr. Catchadourian was using Luke to change herself one last time (into a man), leaving Luca as a woman in the process.\\nComic tensions arise from Luke\\'s efforts to keep Michelle at a distance, Max\\'s defensiveness about his sister, Luca\\'s half-hearted resistance to Max\\'s affections, and Luke\\'s confused aggression toward Max. When Michelle discovers that Luke had sex with Dr. Catchadourian, she feels betrayed. Hoping to find Michelle, Luca seeks out Max...']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "ropes None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ropes (../pretrain_models/huggingface/ropes/plain_text/1.1.0/2ddf8df2070460ad925df98d3b4b3b3809837d246df7c6e4601dca6ce792ebde)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013262510299682617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4470936a8444568ecb386c250a2616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 12, original prompts = 10\n",
      "dict_keys(['id', 'background', 'situation', 'question', 'answers'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['I can use this background: Many chemicals that were once commonly used were later found out to be harmful to the environment , to human health, or both. The element lead was once a common additive to gasoline and to paint. Plumbing pipes were once typically made of lead. Only since the 1970s has the danger of lead become apparent. It causes brain damage and small children (who often chewed on objects painted with lead-based paint) are particularly susceptible. The use of lead in gasoline, paint, and plumbing pipes is now banned and new materials are being developed to replace the hazardous lead components.\\n\\nNow, I have a new situation: There is a study being conducted on a group of small children. Group A is a group from a town that was rated highest quality water in the United States, while Group B is from Flint, Michigan, where it was found that lead was in their water supply.\\n\\nAnswer this question please: Which group has a higher likely hood of brain damage?', 'Group B']\n",
      "=====================\n",
      "{% if answers.text %}\n",
      "{{ situation }}\n",
      "\n",
      "{{ question }}\n",
      "\n",
      "|||\n",
      "{{ answers.text | choice }}\n",
      "{% endif %}\n",
      "=====================\n",
      "['There is a study being conducted on a group of small children. Group A is a group from a town that was rated highest quality water in the United States, while Group B is from Flint, Michigan, where it was found that lead was in their water supply.\\n\\nWhich group has a higher likely hood of brain damage?', 'Group B']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "squad_v2 None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_v2 (../pretrain_models/huggingface/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01378321647644043,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547f8869fd3a4e7187cb9561753775dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 12, original prompts = 4\n",
      "dict_keys(['id', 'title', 'context', 'question', 'answers'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['What is the answer?\\nContext: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.;\\nQuestion: In what country is Normandy located?;\\nAnswer:', 'France']\n",
      "=====================\n",
      "{% if answers.text != [] %}\n",
      "Determine the question that you might have asked to get back the following answer for the given context\n",
      "Context: {{context}};\n",
      "Answer: {{answers.text[0]}};\n",
      "Question: |||\n",
      "{{question}}\n",
      "{% endif %}\n",
      "\n",
      "=====================\n",
      "['Determine the question that you might have asked to get back the following answer for the given context\\nContext: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.;\\nAnswer: France;\\nQuestion:', 'In what country is Normandy located?']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "super_glue record validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (../pretrain_models/huggingface/super_glue/record/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01324009895324707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c1d3f701ef4cef9d33c574bf60086b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 20, original prompts = 18\n",
      "dict_keys(['passage', 'query', 'entities', 'answers', 'idx'])\n",
      "number of choices = 14\n",
      "[\"After reading the article, write another sentence to add to it.\\nTracy Morgan hasn't appeared on stage since the devastating New Jersey crash that nearly ended his life last summer, but all that will change this fall when he returns to host Saturday Night Live. NBC announced on Twitter Monday that Morgan, an SNL alum with seven seasons as a cast member under his belt, will headline the third episode of Season 41 airing October 17. For Morgan, 46, it will be a second time hosting the long-running variety show, the first since the June 2014 pileup on the New Jersey Turnpike that killed his friend and mentor James 'Jimmy Mack' McNair.\\n\\n- \\nMorgan, 46, will host third episode of season 41 of SNL airing October 17\\n\\n- \\nHe tweeted to his fans: 'Stoked to be going home...#SNL'\\n\\n- \\nFor the SNL alum who had spent seven years as cast member, it will be a second time hosting the show\\n\\n- \\nMorgan has been sidelined by severe head trauma suffered in deadly June 2014 crash on New Jersey Turnpike that killed his friend\\n\\n- \\nFirst episode of new SNL season will be hosted by Miley Cyrus, followed by Amy Schumer\", 'On October 10, acclaimed comedian and star of the summer box office hit Trainwreck Amy Schumer will make her SNL debut, followed by Morgan a week later.']\n",
      "14\n",
      "=====================\n",
      "Article:\n",
      "\n",
      "{{ passage.split(\"@highlight\")[0] }}\n",
      "\n",
      "Highlights:\n",
      "\n",
      "- {{ passage.split(\"@highlight\")[1:] | join(\"\\n- \") }} \n",
      "\n",
      " ||| {% if ( answers | length ) > 0 %}- {{ query | replace(\"@placeholder\", answers | choice) }} {% endif %}\n",
      "=====================\n",
      "[\"Article:\\n\\nTracy Morgan hasn't appeared on stage since the devastating New Jersey crash that nearly ended his life last summer, but all that will change this fall when he returns to host Saturday Night Live. NBC announced on Twitter Monday that Morgan, an SNL alum with seven seasons as a cast member under his belt, will headline the third episode of Season 41 airing October 17. For Morgan, 46, it will be a second time hosting the long-running variety show, the first since the June 2014 pileup on the New Jersey Turnpike that killed his friend and mentor James 'Jimmy Mack' McNair.\\n\\n\\nHighlights:\\n\\n- \\nMorgan, 46, will host third episode of season 41 of SNL airing October 17\\n\\n- \\nHe tweeted to his fans: 'Stoked to be going home...#SNL'\\n\\n- \\nFor the SNL alum who had spent seven years as cast member, it will be a second time hosting the show\\n\\n- \\nMorgan has been sidelined by severe head trauma suffered in deadly June 2014 crash on New Jersey Turnpike that killed his friend\\n\\n- \\nFirst episode of new SNL season will be hosted by Miley Cyrus, followed by Amy Schumer\", '- On October 10, acclaimed comedian and star of the summer box office hit Trainwreck Amy Schumer will make her SNL debut, followed by Tracy Morgan a week later.']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "quoref None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quoref (../pretrain_models/huggingface/quoref/default/0.1.0/82bb58a6b25cd8dbb4625a7ba6a5d0a224af1f4d392ca0de8b9e0c23e78557fe)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013266324996948242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1d5b0e314a4e1a946556f1e4abd5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 11, original prompts = 10\n",
      "dict_keys(['id', 'question', 'context', 'title', 'url', 'answers'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[\"A friend asked me to answer this question: What is the first name of the person who purchases a revolver?, using the article: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged.\\nFirst he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted.\\nWith several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges.\\nThat same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit., what would be the answer ?\", 'Frankie']\n",
      "=====================\n",
      "Given the below context:\n",
      "\n",
      "{{context}}\n",
      "\n",
      "Guess a valid title for it! |||\n",
      "{{title}}\n",
      "=====================\n",
      "[\"Given the below context:\\n\\nFrankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged.\\nFirst he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted.\\nWith several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges.\\nThat same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.\\n\\nGuess a valid title for it!\", 'Blast of Silence']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "tydiqa primary_task validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tydiqa (../pretrain_models/huggingface/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013581991195678711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3337411d674fb6b388f7bd2aac7c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 8, original prompts = 5\n",
      "dict_keys(['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'])\n",
      "number of choices = 2\n",
      "['']\n",
      "2\n",
      "=====================\n",
      "{% if language == \"english\" %} \n",
      "    {% if annotations.yes_no_answer[0] == \"YES\" or annotations.yes_no_answer[0] == \"NO\" %} \n",
      "Answer the question about {{document_title}}.\n",
      "Question: {{question_text}}. Yes or No?\n",
      "||| \n",
      "{{annotations.yes_no_answer[0] | capitalize}}\n",
      "    {% endif %}\n",
      "{% endif %}\n",
      "=====================\n",
      "['']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "cos_e v1.11 validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cos_e (../pretrain_models/huggingface/cos_e/v1.11/1.11.0/e8dc57a5b321a2a97063efb8d316d6d8a0d9a2d3a392dafc913e55bed42736d2)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013692378997802734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6590323315f64ef1828559b71f1e8ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 11, original prompts = 6\n",
      "dict_keys(['id', 'question', 'choices', 'answer', 'abstractive_explanation', 'extractive_explanation'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Pick the option in line with common sense to answer the question.\\nQuestion: A beaver is know for building prowess, their supplies come from where?\\nOptions:\\n\\nA. british columbia\\n\\nB. body of water\\n\\nC. wooded area\\n\\nD. pay debts\\n\\nE. zoo', 'C']\n",
      "=====================\n",
      "Here's a question and a few possible answers: \n",
      "\n",
      "Q: {{ question }}\n",
      "Possible A: {{ choices | join(\", \") }}\n",
      "\n",
      "Why is \"{{answer}}\" an answer aligned with human common sense? \n",
      "|||\n",
      "{{ abstractive_explanation }}\n",
      "=====================\n",
      "['Here\\'s a question and a few possible answers: \\n\\nQ: A beaver is know for building prowess, their supplies come from where?\\nPossible A: british columbia, body of water, wooded area, pay debts, zoo\\n\\nWhy is \"wooded area\" an answer aligned with human common sense?', 'Wooden area is only the place for the beaver supplies.']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "cosmos_qa None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset cosmos_qa (../pretrain_models/huggingface/cosmos_qa/default/0.1.0/e4e873eafb86b174fbfdf309a8baac27525753e655fe93f48f99023efa950ae6)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013321161270141602,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0199950213e94800ab3a40b3c43c695e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 13, original prompts = 10\n",
      "dict_keys(['id', 'context', 'question', 'answer0', 'answer1', 'answer2', 'answer3', 'label'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ?\\nAccording to the above context, choose the best option to answer the following question.\\nQuestion: Why is this person asking about divorce ?\\nOptions:\\nA. If he gets married in the church he wo nt have to get a divorce .\\nB. He wants to get married to a different person .\\nC. He wants to know if he does nt like this girl can he divorce her ?\\nD. None of the above choices .', 'B']\n",
      "=====================\n",
      "Based on the context and the answer, generate a question. \n",
      "\n",
      "Context: {{context}}\n",
      "\n",
      "Answer:\n",
      "{% if label == 0 %}\n",
      "{{answer0}}\n",
      "{% elif label == 1 %}\n",
      "{{answer1}}\n",
      "{% elif label == 2 %}\n",
      "{{answer2}}\n",
      "{% elif label == 3 %}\n",
      "{{answer3}}\n",
      "{% endif %}\n",
      "|||\n",
      "{{question}}\n",
      "=====================\n",
      "['Based on the context and the answer, generate a question. \\n\\nContext: Do i need to go for a legal divorce ? I wanted to marry a woman but she is not in the same religion , so i am not concern of the marriage inside church . I will do the marriage registered with the girl who i am going to get married . But legally will there be any complication , like if the other woman comes back one day , will the girl who i am going to get married now will be in trouble or Is there any complication ?\\n\\nAnswer:\\n\\nHe wants to get married to a different person .', 'Why is this person asking about divorce ?']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "dream None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset dream (../pretrain_models/huggingface/dream/plain_text/1.0.0/0835c7949b04e4dc7d094375c7b502ae12c6b17dae8e715d8c363257a391545a)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01415395736694336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed055ef6d7745ada04d89393a83caa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 2\n",
      "dict_keys(['id', 'dialogue_id', 'dialogue', 'question', 'choice', 'answer'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[\"Dialogue:\\n\\nM: How long have you been teaching in this middle school?\\n\\nW: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new.\\n\\nQuestion: What's the woman probably going to do? \\n\\n- To teach a different textbook.\\n\\n- To change her job.\\n\\n- To learn a different textbook.\", 'To change her job.']\n",
      "=====================\n",
      "Given the question \"{{question}}\" and the answer \"{{answer}}\", write a conversation that might have happened.\n",
      "|||\n",
      "{{dialogue | join(\"\\n\\n\")}}\n",
      "=====================\n",
      "['Given the question \"What\\'s the woman probably going to do?\" and the answer \"To change her job.\", write a conversation that might have happened.', \"M: How long have you been teaching in this middle school?\\n\\nW: For ten years. To be frank, I'm tired of teaching the same textbook for so long though I do enjoy being a teacher. I'm considering trying something new.\"]\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "openbookqa main validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset openbookqa (../pretrain_models/huggingface/openbookqa/main/1.0.1/f338ccacfbc86fb8c2de3aa1c06d2ce686933de3bca284dba97d32592c52b33f)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013464212417602539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83d5d8e04df405985b9cd4fcc01715b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 7, original prompts = 7\n",
      "dict_keys(['id', 'question_stem', 'choices', 'answerKey'])\n",
      "number of choices = 4\n",
      "['Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as\\n\\nChoices:\\n- Deep sea animals\\n- fish\\n- Long Sea Fish\\n- Far Sea Animals', 'Deep sea animals']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "qasc None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset qasc (../pretrain_models/huggingface/qasc/default/0.1.0/a8c2ff717429f8f9041f665234865cc42c93d4b1b3c4f16a1e119a85366714ad)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01321721076965332,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9832f59a732a4314aae8172423512171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 8, original prompts = 5\n",
      "dict_keys(['id', 'question', 'choices', 'answerKey', 'fact1', 'fact2', 'combinedfact', 'formatted_question'])\n",
      "number of choices = 2\n",
      "['Climate is generally described in terms of temperature and moisture, and fire behavior is driven by local weather conditions such as winds, temperature and moisture. Given these facts, climate is generally described in terms of what among the following options:\\n- sand\\n - occurs over a wide range\\n - forests\\n - Global warming\\n - rapid changes occur\\n - local weather conditions\\n - measure of motion\\n - city life', 'local weather conditions']\n",
      "2\n",
      "=====================\n",
      "If I tell you that {{combinedfact[0]|capitalize}}{{ combinedfact[1:]|trim('.') }}, and ask you the question \"{{ question[0]|lower }}{{ question[1:] }}\", is the correct answer \"{{ choices.text[0][0]|lower}}{{ choices.text[0][1:]|trim('.') }}\"? \n",
      "\n",
      "||| \n",
      "\n",
      "{% if answerKey == choices.label[0] %} Yes {% else %} No {% endif %}\n",
      "=====================\n",
      "['If I tell you that Climate is generally described in terms of local weather conditions, and ask you the question \"climate is generally described in terms of what?\", is the correct answer \"sand\"?', 'No']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "quail None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset quail (../pretrain_models/huggingface/quail/quail/1.3.0/3cabab19c99e571b528209e14313cfff1debf772db9e24e19b4fcbeb8399336c)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013674497604370117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c861c8bb437493eba2b82220dfc4390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 13, original prompts = 10\n",
      "dict_keys(['id', 'context_id', 'question_id', 'domain', 'metadata', 'context', 'question', 'question_type', 'answers', 'correct_answer_id'])\n",
      "number of choices = 4\n",
      "['Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke.\\nA chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off.\\nShe wrote down the license number as she circled around to the side of the expensive vehicle. He\\'ll have a big wad of cash, she thought.\\nLarry Luzor had just stepped out of the car, when she said, \"Nice car, Honey.\"\\n\"Uh, thanks.\"\\n\"I\\'m Candy. You got a sweet tooth tonight?\"\\nHe gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5\\'8\". The long legs were very nice.\\nLarry had never used a prostitute. He\\'d always thought of it as revolting. The idea of having sex with a woman who\\'d been with hundreds of men did not appeal to him.\\nBut this didn\\'t seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn\\'t. He knew she had to be just as skanky as the rest of them. Still--if he hadn\\'t been in the middle of something important he might have been more than willing to buy what she was selling.\\n\"So, what do you say? Want to get it on?\" She smiled seductively.\\nHe was impressed that she had all her teeth, and that they looked white.\\n\"How can I resist?\" He grinned at her and winked.\\nAccording to the above context, choose the correct option to answer the following question.\\nQuestion: How long was Candy trying to seduce Larry?\\nOptions:\\n\\nA. about 10 minutes\\n\\nB. about 2 hours\\n\\nC. not enough information\\n\\nD. All day', 'A']\n",
      "4\n",
      "=====================\n",
      "{{ context }}\n",
      "According to the above context, answer the following question.\n",
      "{{ question }}\n",
      "|||\n",
      "{{ answer_choices[correct_answer_id] }}\n",
      "=====================\n",
      "['Candy watched the bearded man drive his silver BMW into the convenience store parking lot and pull around to the side, near the back corner of the building. There were plenty of open slots in the front, so she figured the guy was there for something other than a bag of chips and a coke.\\nA chilly breeze blew up her mini-skirt and she shivered. She pressed her legs together tightly to generate some heat. The knee-high boots protected her feet and calves, but her butt was freezing off.\\nShe wrote down the license number as she circled around to the side of the expensive vehicle. He\\'ll have a big wad of cash, she thought.\\nLarry Luzor had just stepped out of the car, when she said, \"Nice car, Honey.\"\\n\"Uh, thanks.\"\\n\"I\\'m Candy. You got a sweet tooth tonight?\"\\nHe gave her the once over. Her black hair framed a pretty, young-looking face. The low-cut blouse left little to the imagination, barely hiding her nipples. She was average height, but the high heel boots elevated her to about 5\\'8\". The long legs were very nice.\\nLarry had never used a prostitute. He\\'d always thought of it as revolting. The idea of having sex with a woman who\\'d been with hundreds of men did not appeal to him.\\nBut this didn\\'t seem like a typical hooker. She seemed too clean--almost pure. But of course, she wasn\\'t. He knew she had to be just as skanky as the rest of them. Still--if he hadn\\'t been in the middle of something important he might have been more than willing to buy what she was selling.\\n\"So, what do you say? Want to get it on?\" She smiled seductively.\\nHe was impressed that she had all her teeth, and that they looked white.\\n\"How can I resist?\" He grinned at her and winked.\\nAccording to the above context, answer the following question.\\nHow long was Candy trying to seduce Larry?', 'about 10 minutes']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "quarel None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quarel (../pretrain_models/huggingface/quarel/default/0.1.0/acb6fe5b05f9a6884ba37d378738bced9f1e96843157521e7cd854f9f6618c5d)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013712644577026367,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ceb2c3dc43648f09b9b698b4aadbd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 0\n",
      "dict_keys(['id', 'answer_index', 'logical_forms', 'logical_form_pretty', 'world_literals', 'question'])\n",
      "number of choices = 2\n",
      "2\n",
      "=====================\n",
      "Choose between \"{{answer_choices[0]}}\" and  \"{{answer_choices[1]}}\".\n",
      "Question: {{question}}\n",
      "|||\n",
      "{{answer_choices[answer_index]}}\n",
      "=====================\n",
      "['Choose between \"marble floor\" and  \"wet floor\".\\nQuestion: Tommy glided across the marble floor with ease, but slipped and fell on the wet floor because _____ has more resistance. (A) marble floor (B) wet floor', 'wet floor']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "quartz None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quartz (../pretrain_models/huggingface/quartz/default/0.1.0/6e5195fb88ecd7a75eda5d8f3549c262c8b15267366f38f9c153f40da92724a6)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01623058319091797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf4cb35cca74fcb811f654dc3aa81a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 8, original prompts = 8\n",
      "dict_keys(['id', 'question', 'choices', 'answerKey', 'para', 'para_id', 'para_anno', 'question_anno'])\n",
      "number of choices = 2\n",
      "['Answer the question based on the following text.\\n\\nQuestion:\\n\\n\\nIf Jim moves some particles of matter farther apart, what will happen to the rate at which they can pass vibrations on to nearby particles decrease or increase? \\n\\n\\nText:\\n\\nWhen particles of matter are closer together, they can more quickly pass the energy of vibrations to nearby particles.', 'decrease']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "race high validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset race (../pretrain_models/huggingface/race/high/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013858795166015625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6301a833daf24f5a9e9165f876b9ac36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset race (../pretrain_models/huggingface/race/middle/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 8, original prompts = 5\n",
      "dict_keys(['example_id', 'article', 'answer', 'question', 'options'])\n",
      "number of choices = 2\n",
      "[\"Read the following article and answer the question.\\nArticle: I am a psychologist. I first met Timothy, a quiet, overweight eleven-year-old boy, when his mother brought him to me to discuss his declining grades. A few minutes with Timothy were enough to confirm that his self-esteem  and general happiness were falling right along with _ . I asked about Timothy's typical day. He awoke every morning at six thirty so he could reach his school by eight and arrived home around four thirty each afternoon. He then had a quick snack, followed by either a piano lesson or a lesson with his math tutor. He finished dinner at 7 pm, and then he sat down to do homework for two to three hours. Quickly doing the math in my head, I found that Timothy spent an average of thirteen hours a day at a writing desk.\\nWhat if Timothy spent thirteen hours a day at a sewing machine instead of a desk? We would immediately be shocked, because that would be called children being horribly mistreated. Timothy was far from being mistreated, but the mountain of homework he faced daily resulted in a similar consequence --he was being robbed of his childhood. In fact, Timothy had no time to do anything he truly enjoyed, such as playing video games, watching movies, or playing board games with his friends.\\nPlay, however, is a crucial part of healthy child development. It affects children's creativity, their social skills, and even their brain development. The absence of play, physical exercise, and freefrom social interaction takes a serious toll on many children. It can also cause significant health problems like childhood obesity, sleep problems and depression.\\nExperts in the field recommend the minutes children spend on their homework should be no more than ten times the number of their grade level. As a fifthgrader, Timothy should have no more than fifty minutes a day of homework (instead of three times that amount). Having an extra two hours an evening to play, relax, or see a friend would soundly benefit any child's life quality.\\nQuestion: What did the writer think of Timothy after learning about his typical day?\\nAnswer:\", 'Timothy had a heavy burden.']\n",
      "2\n",
      "=====================\n",
      "{% set candidate = [\"A\", \"B\", \"C\", \"D\"] | choice %}\n",
      "Article: {{article}}\n",
      "Question: {{question}}\n",
      "Yes or no, is the answer \"{{ [options.0,options.1,options.2,options.3][{\"A\":0,\"B\":1,\"C\":2,\"D\":3}[answer]] }}\"?\n",
      "|||\n",
      "{% if candidate == answer %}\n",
      "Yes\n",
      "{% else %}\n",
      "No\n",
      "{% endif %}\n",
      "=====================\n",
      "['Article: I am a psychologist. I first met Timothy, a quiet, overweight eleven-year-old boy, when his mother brought him to me to discuss his declining grades. A few minutes with Timothy were enough to confirm that his self-esteem  and general happiness were falling right along with _ . I asked about Timothy\\'s typical day. He awoke every morning at six thirty so he could reach his school by eight and arrived home around four thirty each afternoon. He then had a quick snack, followed by either a piano lesson or a lesson with his math tutor. He finished dinner at 7 pm, and then he sat down to do homework for two to three hours. Quickly doing the math in my head, I found that Timothy spent an average of thirteen hours a day at a writing desk.\\nWhat if Timothy spent thirteen hours a day at a sewing machine instead of a desk? We would immediately be shocked, because that would be called children being horribly mistreated. Timothy was far from being mistreated, but the mountain of homework he faced daily resulted in a similar consequence --he was being robbed of his childhood. In fact, Timothy had no time to do anything he truly enjoyed, such as playing video games, watching movies, or playing board games with his friends.\\nPlay, however, is a crucial part of healthy child development. It affects children\\'s creativity, their social skills, and even their brain development. The absence of play, physical exercise, and freefrom social interaction takes a serious toll on many children. It can also cause significant health problems like childhood obesity, sleep problems and depression.\\nExperts in the field recommend the minutes children spend on their homework should be no more than ten times the number of their grade level. As a fifthgrader, Timothy should have no more than fifty minutes a day of homework (instead of three times that amount). Having an extra two hours an evening to play, relax, or see a friend would soundly benefit any child\\'s life quality.\\nQuestion: What did the writer think of Timothy after learning about his typical day?\\nYes or no, is the answer \"Timothy had a heavy burden.\"?', 'No']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "race middle validation\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01368260383605957,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03c8520f3554aa3b1ecf21d2cca1e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 8, original prompts = 5\n",
      "dict_keys(['example_id', 'article', 'answer', 'question', 'options'])\n",
      "number of choices = 2\n",
      "['Read the following article and answer the question.\\nArticle: APRIL 5 marks the 100thanniversary of the sinking of the Titanic. In 1997, the movieTitanicwas a huge hit all around the world. Now, the 3-D version of the movie will come to Chinese theaters on April 10.\\nAs anyone who has seen the movie knows, the Titanic struck an iceberg  and sank on its first _ in 1912, killing 1,517 people. A century after the accident, scientists have found something new to blame  for the sinking: the moon. Donald Olson, a Texas State University physicist, led a team of astronomers  to examine the moon\\'s role, according to Reuters.\\nNormally, icebergs stay in place and cannot move until they melt enough or a high tide  frees them. A single iceberg can become stuck many times on its journey southward. The process can take several years.\\nAccording to Olson, a hundred years ago the moon made its closest approach to the Earth in more than 1,400 years. This caused the moon to have a much stronger pull on the Earth\\'s oceans than usual, which created a super-high tide. The tide pushed icebergs from shallow waters off the coasts of Canada\\'s provinces, Newfoundland and Labrador, into the Titanic\\'s way.\\n\"Of course, the final cause of the accident was that the ship struck an iceberg,\" Olson told Reuters. \"It went full speed into a region with icebergs, but thelunar connection  may explain how an unusually large number of icebergs got into the path of the Titanic.\"\\nThe research team will publish their research in the April issue of Sky & Telescope magazine.\\n,,.\\nQuestion: How many people lost their lives in this accident?\\nAnswer:', 'More than 1,500 people.']\n",
      "2\n",
      "=====================\n",
      "{% set candidate = [\"A\", \"B\", \"C\", \"D\"] | choice %}\n",
      "Article: {{article}}\n",
      "Question: {{question}}\n",
      "Yes or no, is the answer \"{{ [options.0,options.1,options.2,options.3][{\"A\":0,\"B\":1,\"C\":2,\"D\":3}[answer]] }}\"?\n",
      "|||\n",
      "{% if candidate == answer %}\n",
      "Yes\n",
      "{% else %}\n",
      "No\n",
      "{% endif %}\n",
      "=====================\n",
      "['Article: APRIL 5 marks the 100thanniversary of the sinking of the Titanic. In 1997, the movieTitanicwas a huge hit all around the world. Now, the 3-D version of the movie will come to Chinese theaters on April 10.\\nAs anyone who has seen the movie knows, the Titanic struck an iceberg  and sank on its first _ in 1912, killing 1,517 people. A century after the accident, scientists have found something new to blame  for the sinking: the moon. Donald Olson, a Texas State University physicist, led a team of astronomers  to examine the moon\\'s role, according to Reuters.\\nNormally, icebergs stay in place and cannot move until they melt enough or a high tide  frees them. A single iceberg can become stuck many times on its journey southward. The process can take several years.\\nAccording to Olson, a hundred years ago the moon made its closest approach to the Earth in more than 1,400 years. This caused the moon to have a much stronger pull on the Earth\\'s oceans than usual, which created a super-high tide. The tide pushed icebergs from shallow waters off the coasts of Canada\\'s provinces, Newfoundland and Labrador, into the Titanic\\'s way.\\n\"Of course, the final cause of the accident was that the ship struck an iceberg,\" Olson told Reuters. \"It went full speed into a region with icebergs, but thelunar connection  may explain how an unusually large number of icebergs got into the path of the Titanic.\"\\nThe research team will publish their research in the April issue of Sky & Telescope magazine.\\n,,.\\nQuestion: How many people lost their lives in this accident?\\nYes or no, is the answer \"More than 1,500 people.\"?', 'No']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "sciq None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset sciq (../pretrain_models/huggingface/sciq/default/0.1.0/50e5c6e3795b55463819d399ec417bfd4c3c621105e00295ddb5f3633d708493)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015313863754272461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2544ec3de14accbfaab670311bc72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 4\n",
      "dict_keys(['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'])\n",
      "number of choices = 4\n",
      "['Answer the following question given this paragraph: \\n\\n\\n\\n\\nQ: Who proposed the theory of evolution by natural selection?\\n\\n\\nA:', 'darwin']\n",
      "4\n",
      "=====================\n",
      "{% set order = [[0, 1, 2, 3], [0, 1, 3, 2], [0, 2, 1, 3], [0, 2, 3, 1], [0, 3, 1, 2], [0, 3, 2, 1],\n",
      "                             [1, 0, 2, 3], [1, 0, 3, 2], [1, 2, 0, 3], [1, 2, 3, 0], [1, 3, 0, 2], [1, 3, 2, 0],\n",
      "                             [2, 1, 0, 3], [2, 1, 0, 2], [2, 0, 1, 3], [2, 0, 3, 1], [2, 3, 1, 0], [2, 3, 0, 1],\n",
      "                             [3, 1, 2, 0], [3, 1, 0, 2], [3, 2, 1, 0], [3, 2, 0, 1], [3, 0, 1, 2], [3, 0, 2, 1]] | choice %}\n",
      "Q: {{question}}\n",
      "\n",
      "\n",
      " Choices:\n",
      "\n",
      "- {{ answer_choices[order[0]] }}\n",
      "\n",
      "- {{ answer_choices[order[1]] }}\n",
      "\n",
      "- {{ answer_choices[order[2]] }}\n",
      "\n",
      "- {{ answer_choices[order[3]] }}\n",
      "\n",
      "A:|||{{answer_choices[3]}}\n",
      "=====================\n",
      "['Q: Who proposed the theory of evolution by natural selection?\\n\\n\\n Choices:\\n\\n- shaw\\n\\n- Scopes\\n\\n- Linnaeus\\n\\n- darwin\\n\\nA:', 'darwin']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "social_i_qa None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset social_i_qa (../pretrain_models/huggingface/social_i_qa/default/0.1.0/674d85e42ac7430d3dcd4de7007feaffcb1527c535121e09bab2803fbcc925f8)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013632774353027344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f0ce589f34e65a2f54f4f0fc63ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (../pretrain_models/huggingface/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 6, original prompts = 5\n",
      "dict_keys(['context', 'question', 'answerA', 'answerB', 'answerC', 'label'])\n",
      "number of choices = 2\n",
      "['Tracy didn\\'t go home that evening and resisted Riley\\'s attacks.\\n\\nGiven the question \"What does Tracy need to do before this?\", is \"make a new plan\" a valid answer?', 'No']\n",
      "=====================\n",
      "{{context}}\n",
      "\n",
      "Given that the answer to a question is \"{{{\"1\": answerA, \"2\": answerB, \"3\": answerC}[label]}}\", what is the question?\n",
      "\n",
      "|||\n",
      "\n",
      "{{question}}\n",
      "=====================\n",
      "['Tracy didn\\'t go home that evening and resisted Riley\\'s attacks.\\n\\nGiven that the answer to a question is \"Find somewhere to go\", what is the question?', 'What does Tracy need to do before this?']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "super_glue boolq validation\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013892412185668945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e07ad5b05249779c21d356be21cf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (../pretrain_models/huggingface/super_glue/multirc/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 10, original prompts = 10\n",
      "dict_keys(['question', 'passage', 'idx', 'label'])\n",
      "number of choices = 2\n",
      "[\"Ethanol fuel -- All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline. \\nQuestion: does ethanol take more energy make that produces\\nAnswer:\", 'No']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "super_glue multirc validation\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014139652252197266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278fdddbbcb14077abeac4b9bb5ffad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 10, original prompts = 10\n",
      "dict_keys(['paragraph', 'question', 'answer', 'idx', 'label'])\n",
      "number of choices = 2\n",
      "['What causes a change in motion? The application of a force. Any time an object changes motion, a force has been applied. In what ways can this happen? Force can cause an object at rest to start moving. Forces can cause objects to speed up or slow down. Forces can cause a moving object to stop. Forces can also cause a change in direction. In short, forces cause changes in motion. The moving object may change its speed, its direction, or both. We know that changes in motion require a force. We know that the size of the force determines the change in motion. How much an objects motion changes when a force is applied depends on two things. It depends on the strength of the force. It also depends on the objects mass. Think about some simple tasks you may regularly do. You may pick up a baseball. This requires only a very small force. \\nWould the mass of a baseball affect how much force you have to use to pick it up? \\nI was going to say \"No\". Does that sound right?', 'No']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "wiki_hop original validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wiki_hop (../pretrain_models/huggingface/wiki_hop/original/1.0.0/eb7c77aeecd79f7ef05ad3d29d48b68998c2b51496a05fe4767fcd01785d8a16)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014090299606323242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1528ba0b8641978db0b0950819c4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 9, original prompts = 5\n",
      "dict_keys(['id', 'question', 'answer', 'candidates', 'supports', 'annotations'])\n",
      "number of choices = 18\n",
      "['Information:\\n\\n- The North German Confederation was a confederation of 22 previously independent states of northern Germany, with nearly 30 million inhabitants. It was the first modern German nation state and the basis for the later German Empire (18711918), when several south German states such as Bavaria joined.\\n\\n- Weimar Republic is an unofficial, historical designation for the German state between 1919 and 1933. The name derives from the city of Weimar, where its constitutional assembly first took place. The official name of the state was still \"Deutsches Reich\"; it had remained unchanged since 1871. In English the country was usually known simply as Germany.\\nA national assembly was convened in Weimar, where a new constitution for the \"Deutsches Reich\" was written, and adopted on 11 August 1919. In its fourteen years, the Weimar Republic faced numerous problems, including hyperinflation, political extremism (with paramilitaries  both left- and right-wing); and contentious relationships with the victors of the First World War. The people of Germany blamed the Weimar Republic rather than their wartime leaders for the country\\'s defeat and for the humiliating terms of the Treaty of Versailles. However, the Weimar Republic government successfully reformed the currency, unified tax policies, and organized the railway system. Weimar Germany eliminated most of the requirements of the Treaty of Versailles; it never completely met its disarmament requirements, and eventually paid only a small portion of the war reparations (by twice restructuring its debt through the Dawes Plan and the Young Plan). Under the Locarno Treaties, Germany accepted the western borders of the republic, but continued to dispute the Eastern border.\\n\\n- The dreadnought was the predominant type of battleship in the early 20th century. The first of its kind, the Royal Navy\\'s , made such a strong impression on people\\'s minds when launched in 1906 that similar battleships built subsequently were referred to generically as \"dreadnoughts\", and earlier battleships became known as \"pre-dreadnoughts\". \"Dreadnought\"s design had two revolutionary features: an \"all-big-gun\" armament scheme, with more heavy-calibre guns than previous ships, and steam turbine propulsion. As dreadnoughts became a symbol of national power, the arrival of these new warships was a crucial catalyst in the intensifying naval arms race between the United Kingdom and Germany. With the launch of a single ship, \"Dreadnought\", the scales of naval power were reset overnight. As a result, dreadnought races sprang up around the world, including in South America, during the lead up to World War I. Successive designs increased rapidly in size and made use of improvements in armament, armour, and propulsion throughout the dreadnought era. Within five years, new battleships had outclassed \"Dreadnought\". These more powerful vessels were known as \"super-dreadnoughts.\" Most of the original dreadnoughts were scrapped after the end of World War I under the terms of the Washington Naval Treaty, but many of the newer super-dreadnoughts continued to be used throughout World War II. The only surviving dreadnought is , located near the San Jacinto Battleground State Historic Site.\\n\\n- SMS Braunschweig was the first of five pre-dreadnought battleships of the Braunschweig class in the German Kaiserliche Marine ( Imperial Navy ) . She was laid down in 1901 and commissioned in October 1904 , at a cost of 23,983,000 marks . She was named after the then Duchy of Brunswick ( German : Braunschweig ) . Her sister ships were Elsass , Hessen , Preussen , and Lothringen . The ship served in the II Squadron of the German fleet after commissioning , though by the outbreak of World War I she had been moved to the IV Squadron . Braunschweig saw action in the Baltic Sea against the Russian Navy . In August 1915 , the ship participated in the Battle of the Gulf of Riga , during which she engaged the Russian battleship Slava . In 1916 , the ship was placed in reserve owing to crew shortages . She spent the remainder of World War I as a training ship , and after 1917 , as a barracks ship for U-boat crews . Under the terms of the Treaty of Versailles , she was retained after the end of the war and modernized in 1921 -- 22 . Braunschweig served in the reformed Reichsmarine as a coastal defense ship until 1926 , when she was again placed in reserve . She was stricken in 1931 and subsequently broken up for scrap .\\n\\n- World War I (WWI or WW1), also known as the First World War, or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. More than 70\\xa0million military personnel, including 60 million Europeans, were mobilised in one of the largest wars in history. Over nine million combatants and seven million civilians died as a result of the war (including the victims of a number of genocides), a casualty rate exacerbated by the belligerents\\' technological and industrial sophistication, and the tactical stalemate caused by gruelling trench warfare. It was one of the deadliest conflicts in history, and paved the way for major political changes, including revolutions in many of the nations involved.\\n\\n- The Free State of Brunswick was a state of the German Reich in the time of the Weimar Republic. It was formed after the abolition of the Duchy of Brunswick in the course of the German Revolution of 191819. Its capital was Braunschweig (Brunswick).\\n\\n- The Congress of Vienna (German: \"Wiener Kongress\") was a conference of ambassadors of European states chaired by Austrian statesman Klemens von Metternich, and held in Vienna from November 1814 to June 1815, though the delegates had arrived and were already negotiating by late September 1814. The objective of the Congress was to provide a long-term peace plan for Europe by settling critical issues arising from the French Revolutionary Wars and the Napoleonic Wars. The goal was not simply to restore old boundaries but to resize the main powers so they could balance each other off and remain at peace. The leaders were conservatives with little use for republicanism or revolution, both of which threatened to upset the status quo in Europe. France lost all its recent conquests, while Prussia, Austria and Russia made major territorial gains. Prussia added smaller German states in the west, Swedish Pomerania and 60% of the Kingdom of Saxony; Austria gained Venice and much of northern Italy. Russia gained parts of Poland. The new Kingdom of the Netherlands had been created just months before, and included formerly Austrian territory that in 1830 became Belgium.\\nThe immediate background was Napoleonic France\\'s defeat and surrender in May 1814, which brought an end to twenty-five years of nearly continuous war. Negotiations continued despite the outbreak of fighting triggered by Napoleon\\'s dramatic return from exile and resumption of power in France during the Hundred Days of MarchJuly 1815. The Congress\\'s \"Final Act\" was signed nine days before his final defeat at Waterloo on 18 June 1815.\\n\\n- The German Confederation was an association of 39 German states in Central Europe, created by the Congress of Vienna in 1815 to coordinate the economies of separate German-speaking countries and to replace the former Holy Roman Empire. Most historians have judged the Confederation to have been weak and ineffective, as well as an obstacle to the creation of a German nation-state. It collapsed due to the rivalry between Prussia and Austria, warfare, the 1848 revolution, and the inability of the multiple members to compromise.\\n\\n- The Royal Navy (RN) is the United Kingdom\\'s naval warfare force. Although warships were used by the English kings from the early medieval period, the first major maritime engagements were fought in the Hundred Years War against the kingdom of France. The modern Royal Navy traces its origins to the early 16th century; the oldest of the UK\\'s armed services, it is known as the Senior Service.\\n\\n- Wolfenbüttel is a town in Lower Saxony, Germany, the administrative capital of Wolfenbüttel District. It is best known as the location of the internationally renowned Herzog August Library and for having the largest concentration of timber-framed buildings in Germany. It is an episcopal see of the Evangelical Lutheran Church in Brunswick. It is also home to the Jägermeister distillery and houses a campus of the Ostfalia University of Applied Sciences.\\n\\n- The German Empire (officially \\') was the historical German nation state that existed from the unification of Germany in 1871 to the abdication of Kaiser Wilhelm II in November 1918, when Germany became a federal republic (the Weimar Republic).\\n\\n- Braunschweig (Low German: \"Brunswiek\" ), also called Brunswick in English, is a city of 252,768 people (as of 31 December 2015), in the state of Lower Saxony, Germany. It is located north of the Harz mountains at the furthest navigable point of the Oker river, which connects to the North Sea via the rivers Aller and Weser. A powerful and influential centre of commerce in medieval Germany, Braunschweig was a member of the Hanseatic League from the 13th until the 17th century, and the capital of the state of Brunswick until its disestablishment in 1946. Today, Braunschweig is the second largest city in Lower Saxony and a major centre of scientific research and development.\\n\\n- The Washington Naval Treaty, also known as the Five-Power Treaty, and including the \"Four-Power Treaty\" and the \"Nine-power Treaty\" was a treaty among the major nations that had won World War I, which agreed to prevent an arms race by limiting naval construction. It was negotiated at the Washington Naval Conference, held in Washington, D.C., from November 1921 to February 1922, and it was signed by the governments of the United Kingdom, the United States, Japan, France, and Italy. It limited the construction of battleships, battlecruisers and aircraft carriers by the signatories. The numbers of other categories of warships, including cruisers, destroyers and submarines, were not limited by the treaty, but those ships were limited to 10,000 tons displacement.\\n\\n- The Duchy of Brunswick was a historical German state. Its capital was the city of Brunswick (\"Braunschweig\").\\nIt was established as the successor state of the Principality of Brunswick-Wolfenbüttel by the Congress of Vienna in 1815. In the course of the 19th-century history of Germany, the duchy was part of the German Confederation, the North German Confederation and from 1871 the German Empire. It was disestablished after the end of World War I, its territory incorporated into the Weimar Republic as the Free State of Brunswick.\\n\\n- World War\\xa0II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945, although related conflicts began earlier. It involved the vast majority of the world\\'s nationsincluding all of the great powerseventually forming two opposing military alliances: the Allies and the Axis. It was the most widespread war in history, and directly involved more than 100 million people from over 30 countries. In a state of \"total war\", the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, erasing the distinction between civilian and military resources. Marked by mass deaths of civilians, including the Holocaust (in which approximately 11 million people were killed) and the strategic bombing of industrial and population centres (in which approximately one million were killed, and which included the atomic bombings of Hiroshima and Nagasaki), it resulted in an estimated 50 million to 85 million fatalities. These made World War\\xa0II the deadliest conflict in human history.\\n\\n\\n\\nGiven the information above, choose from the list below the object entity that exhibits the relation \\'country\\' with the subject \\'sms braunschweig\\'.\\n\\nChoices:\\n- austria\\n - duchy of brunswick\\n - france\\n - german confederation\\n - german empire\\n - germany\\n - holy roman empire\\n - italy\\n - japan\\n - kingdom of france\\n - kingdom of saxony\\n - northern italy\\n - roman empire\\n - russia\\n - saxony\\n - united kingdom\\n - weimar republic\\n - world', 'german empire']\n",
      "=====================\n",
      "Information:\n",
      "{% for support in supports %}\n",
      "- {{ support }}\n",
      "{% endfor %}\n",
      "\n",
      "{% set question_split = question.split(' ') %}\n",
      "What is the relationship between '{{ question_split[1:] | join(\" \")}}' and '{{answer}}'?\n",
      "\n",
      "|||\n",
      "{{ question_split[0] | replace(\"_\", \" \") }}\n",
      "=====================\n",
      "['Information:\\n\\n- The North German Confederation was a confederation of 22 previously independent states of northern Germany, with nearly 30 million inhabitants. It was the first modern German nation state and the basis for the later German Empire (18711918), when several south German states such as Bavaria joined.\\n\\n- Weimar Republic is an unofficial, historical designation for the German state between 1919 and 1933. The name derives from the city of Weimar, where its constitutional assembly first took place. The official name of the state was still \"Deutsches Reich\"; it had remained unchanged since 1871. In English the country was usually known simply as Germany.\\nA national assembly was convened in Weimar, where a new constitution for the \"Deutsches Reich\" was written, and adopted on 11 August 1919. In its fourteen years, the Weimar Republic faced numerous problems, including hyperinflation, political extremism (with paramilitaries  both left- and right-wing); and contentious relationships with the victors of the First World War. The people of Germany blamed the Weimar Republic rather than their wartime leaders for the country\\'s defeat and for the humiliating terms of the Treaty of Versailles. However, the Weimar Republic government successfully reformed the currency, unified tax policies, and organized the railway system. Weimar Germany eliminated most of the requirements of the Treaty of Versailles; it never completely met its disarmament requirements, and eventually paid only a small portion of the war reparations (by twice restructuring its debt through the Dawes Plan and the Young Plan). Under the Locarno Treaties, Germany accepted the western borders of the republic, but continued to dispute the Eastern border.\\n\\n- The dreadnought was the predominant type of battleship in the early 20th century. The first of its kind, the Royal Navy\\'s , made such a strong impression on people\\'s minds when launched in 1906 that similar battleships built subsequently were referred to generically as \"dreadnoughts\", and earlier battleships became known as \"pre-dreadnoughts\". \"Dreadnought\"s design had two revolutionary features: an \"all-big-gun\" armament scheme, with more heavy-calibre guns than previous ships, and steam turbine propulsion. As dreadnoughts became a symbol of national power, the arrival of these new warships was a crucial catalyst in the intensifying naval arms race between the United Kingdom and Germany. With the launch of a single ship, \"Dreadnought\", the scales of naval power were reset overnight. As a result, dreadnought races sprang up around the world, including in South America, during the lead up to World War I. Successive designs increased rapidly in size and made use of improvements in armament, armour, and propulsion throughout the dreadnought era. Within five years, new battleships had outclassed \"Dreadnought\". These more powerful vessels were known as \"super-dreadnoughts.\" Most of the original dreadnoughts were scrapped after the end of World War I under the terms of the Washington Naval Treaty, but many of the newer super-dreadnoughts continued to be used throughout World War II. The only surviving dreadnought is , located near the San Jacinto Battleground State Historic Site.\\n\\n- SMS Braunschweig was the first of five pre-dreadnought battleships of the Braunschweig class in the German Kaiserliche Marine ( Imperial Navy ) . She was laid down in 1901 and commissioned in October 1904 , at a cost of 23,983,000 marks . She was named after the then Duchy of Brunswick ( German : Braunschweig ) . Her sister ships were Elsass , Hessen , Preussen , and Lothringen . The ship served in the II Squadron of the German fleet after commissioning , though by the outbreak of World War I she had been moved to the IV Squadron . Braunschweig saw action in the Baltic Sea against the Russian Navy . In August 1915 , the ship participated in the Battle of the Gulf of Riga , during which she engaged the Russian battleship Slava . In 1916 , the ship was placed in reserve owing to crew shortages . She spent the remainder of World War I as a training ship , and after 1917 , as a barracks ship for U-boat crews . Under the terms of the Treaty of Versailles , she was retained after the end of the war and modernized in 1921 -- 22 . Braunschweig served in the reformed Reichsmarine as a coastal defense ship until 1926 , when she was again placed in reserve . She was stricken in 1931 and subsequently broken up for scrap .\\n\\n- World War I (WWI or WW1), also known as the First World War, or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. More than 70\\xa0million military personnel, including 60 million Europeans, were mobilised in one of the largest wars in history. Over nine million combatants and seven million civilians died as a result of the war (including the victims of a number of genocides), a casualty rate exacerbated by the belligerents\\' technological and industrial sophistication, and the tactical stalemate caused by gruelling trench warfare. It was one of the deadliest conflicts in history, and paved the way for major political changes, including revolutions in many of the nations involved.\\n\\n- The Free State of Brunswick was a state of the German Reich in the time of the Weimar Republic. It was formed after the abolition of the Duchy of Brunswick in the course of the German Revolution of 191819. Its capital was Braunschweig (Brunswick).\\n\\n- The Congress of Vienna (German: \"Wiener Kongress\") was a conference of ambassadors of European states chaired by Austrian statesman Klemens von Metternich, and held in Vienna from November 1814 to June 1815, though the delegates had arrived and were already negotiating by late September 1814. The objective of the Congress was to provide a long-term peace plan for Europe by settling critical issues arising from the French Revolutionary Wars and the Napoleonic Wars. The goal was not simply to restore old boundaries but to resize the main powers so they could balance each other off and remain at peace. The leaders were conservatives with little use for republicanism or revolution, both of which threatened to upset the status quo in Europe. France lost all its recent conquests, while Prussia, Austria and Russia made major territorial gains. Prussia added smaller German states in the west, Swedish Pomerania and 60% of the Kingdom of Saxony; Austria gained Venice and much of northern Italy. Russia gained parts of Poland. The new Kingdom of the Netherlands had been created just months before, and included formerly Austrian territory that in 1830 became Belgium.\\nThe immediate background was Napoleonic France\\'s defeat and surrender in May 1814, which brought an end to twenty-five years of nearly continuous war. Negotiations continued despite the outbreak of fighting triggered by Napoleon\\'s dramatic return from exile and resumption of power in France during the Hundred Days of MarchJuly 1815. The Congress\\'s \"Final Act\" was signed nine days before his final defeat at Waterloo on 18 June 1815.\\n\\n- The German Confederation was an association of 39 German states in Central Europe, created by the Congress of Vienna in 1815 to coordinate the economies of separate German-speaking countries and to replace the former Holy Roman Empire. Most historians have judged the Confederation to have been weak and ineffective, as well as an obstacle to the creation of a German nation-state. It collapsed due to the rivalry between Prussia and Austria, warfare, the 1848 revolution, and the inability of the multiple members to compromise.\\n\\n- The Royal Navy (RN) is the United Kingdom\\'s naval warfare force. Although warships were used by the English kings from the early medieval period, the first major maritime engagements were fought in the Hundred Years War against the kingdom of France. The modern Royal Navy traces its origins to the early 16th century; the oldest of the UK\\'s armed services, it is known as the Senior Service.\\n\\n- Wolfenbüttel is a town in Lower Saxony, Germany, the administrative capital of Wolfenbüttel District. It is best known as the location of the internationally renowned Herzog August Library and for having the largest concentration of timber-framed buildings in Germany. It is an episcopal see of the Evangelical Lutheran Church in Brunswick. It is also home to the Jägermeister distillery and houses a campus of the Ostfalia University of Applied Sciences.\\n\\n- The German Empire (officially \\') was the historical German nation state that existed from the unification of Germany in 1871 to the abdication of Kaiser Wilhelm II in November 1918, when Germany became a federal republic (the Weimar Republic).\\n\\n- Braunschweig (Low German: \"Brunswiek\" ), also called Brunswick in English, is a city of 252,768 people (as of 31 December 2015), in the state of Lower Saxony, Germany. It is located north of the Harz mountains at the furthest navigable point of the Oker river, which connects to the North Sea via the rivers Aller and Weser. A powerful and influential centre of commerce in medieval Germany, Braunschweig was a member of the Hanseatic League from the 13th until the 17th century, and the capital of the state of Brunswick until its disestablishment in 1946. Today, Braunschweig is the second largest city in Lower Saxony and a major centre of scientific research and development.\\n\\n- The Washington Naval Treaty, also known as the Five-Power Treaty, and including the \"Four-Power Treaty\" and the \"Nine-power Treaty\" was a treaty among the major nations that had won World War I, which agreed to prevent an arms race by limiting naval construction. It was negotiated at the Washington Naval Conference, held in Washington, D.C., from November 1921 to February 1922, and it was signed by the governments of the United Kingdom, the United States, Japan, France, and Italy. It limited the construction of battleships, battlecruisers and aircraft carriers by the signatories. The numbers of other categories of warships, including cruisers, destroyers and submarines, were not limited by the treaty, but those ships were limited to 10,000 tons displacement.\\n\\n- The Duchy of Brunswick was a historical German state. Its capital was the city of Brunswick (\"Braunschweig\").\\nIt was established as the successor state of the Principality of Brunswick-Wolfenbüttel by the Congress of Vienna in 1815. In the course of the 19th-century history of Germany, the duchy was part of the German Confederation, the North German Confederation and from 1871 the German Empire. It was disestablished after the end of World War I, its territory incorporated into the Weimar Republic as the Free State of Brunswick.\\n\\n- World War\\xa0II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945, although related conflicts began earlier. It involved the vast majority of the world\\'s nationsincluding all of the great powerseventually forming two opposing military alliances: the Allies and the Axis. It was the most widespread war in history, and directly involved more than 100 million people from over 30 countries. In a state of \"total war\", the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, erasing the distinction between civilian and military resources. Marked by mass deaths of civilians, including the Holocaust (in which approximately 11 million people were killed) and the strategic bombing of industrial and population centres (in which approximately one million were killed, and which included the atomic bombings of Hiroshima and Nagasaki), it resulted in an estimated 50 million to 85 million fatalities. These made World War\\xa0II the deadliest conflict in human history.\\n\\n\\n\\nWhat is the relationship between \\'sms braunschweig\\' and \\'german empire\\'?', 'country']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "wiqa None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiqa (../pretrain_models/huggingface/wiqa/default/0.1.0/2e512659fd13a0ed50cbdf286b3a963080a7d4389d3e9818b1ae66e5dc83a58b)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013700008392333984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274b4ec036f247bca1e86de0652bc058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 8, original prompts = 2\n",
      "dict_keys(['question_stem', 'question_para_step', 'answer_label', 'answer_label_as_choice', 'choices', 'metadata_question_id', 'metadata_graph_id', 'metadata_para_id', 'metadata_question_type', 'metadata_path_len'])\n",
      "number of choices = 2\n",
      "['Process:\\n- Squirrels try to eat as much as possible\\n- Squirrel gains weight and fat\\n- Squirrel also hides food in or near its den\\n- Squirrels also grow a thicker coat as the weather gets colder\\n- Squirrel lives off of its excess body fat\\n- Squirrel uses its food stores in the winter.\\n\\nQuestion:\\nsuppose squirrels get sick happens, how will it affect squirrels need more food.\\n\\n- A: more\\n- B: less\\n- C: no effect', 'A']\n",
      "2\n",
      "=====================\n",
      "Process:\n",
      "\n",
      "- {{ question_para_step | join(\"\\n- \") }}\n",
      "\n",
      "Perturbation hypothesis:\n",
      "{{question_stem}}\n",
      "\n",
      "Does the supposed perturbation have an effect (direct or indirect) on the process?\n",
      "\n",
      "|||\n",
      "\n",
      "{{{\"EXOGENOUS_EFFECT\": \"yes\", \"OUTOFPARA_DISTRACTOR\": \"no\", \"INPARA_EFFECT\": \"yes\"}[metadata_question_type]}}\n",
      "=====================\n",
      "['Process:\\n\\n- Squirrels try to eat as much as possible\\n- Squirrel gains weight and fat\\n- Squirrel also hides food in or near its den\\n- Squirrels also grow a thicker coat as the weather gets colder\\n- Squirrel lives off of its excess body fat\\n- Squirrel uses its food stores in the winter.\\n\\nPerturbation hypothesis:\\nsuppose squirrels get sick happens, how will it affect squirrels need more food.\\n\\nDoes the supposed perturbation have an effect (direct or indirect) on the process?', 'yes']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "piqa None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset piqa (../pretrain_models/huggingface/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013573169708251953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a0d41a0707408f936f3255d2e578ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 11, original prompts = 5\n",
      "dict_keys(['goal', 'sol1', 'sol2', 'label'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[\"Given a goal and 2 solutions, choose the most appropriate solution.\\nGoal: How do I ready a guinea pig cage for it's new occupants?\\n- Solution 1: Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper strips, you will also need to supply it with a water bottle and a food dish.\\n- Solution 2: Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans material, you will also need to supply it with a water bottle and a food dish.\\n\\nAnswer by returning either Solution 1 or Solution 2\", 'Solution 1']\n",
      "=====================\n",
      "Given a goal and a wrong solution, rewrite it to give a correct solution.\n",
      "Goal: {{goal}} \n",
      "Solution: {{[sol1, sol2][1 - label]}}\n",
      "Corrected solution:\n",
      "|||\n",
      "{{[sol1, sol2][label]}}\n",
      "\n",
      "=====================\n",
      "[\"Given a goal and a wrong solution, rewrite it to give a correct solution.\\nGoal: How do I ready a guinea pig cage for it's new occupants? \\nSolution: Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans material, you will also need to supply it with a water bottle and a food dish.\\nCorrected solution:\", 'Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper strips, you will also need to supply it with a water bottle and a food dish.']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "amazon_polarity None test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_polarity (../pretrain_models/huggingface/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013751506805419922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cde905ffa74a25a5c7517f02696371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 9, original prompts = 9\n",
      "dict_keys(['label', 'title', 'content'])\n",
      "number of choices = 2\n",
      "['Is this product review positive?\\nTitle: Great CD\\nReview: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I\\'m in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life\\'s hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\\nAnswer:', 'Yes']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "app_reviews None train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset app_reviews (../pretrain_models/huggingface/app_reviews/default/0.0.0/20335b51b604b9bc04b7be253cd8445caa9ba93f15f39a4b0492b9e9102853de)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013595342636108398,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992506c2a8504878bb3276db24f7d08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (../pretrain_models/huggingface/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 4, original prompts = 0\n",
      "dict_keys(['package_name', 'review', 'date', 'star'])\n",
      "number of choices = 5\n",
      "5\n",
      "=====================\n",
      "Given this review: \"{{review}}\"\n",
      "Would you recommend this app to a friend? {{answer_choices[0]}}, {{answer_choices[1]}}, {{answer_choices[2]}}, {{answer_choices[3]}}, or {{answer_choices[4]}}?\n",
      "|||\n",
      "{{answer_choices[star-1]}}\n",
      "=====================\n",
      "['Given this review: \"Great app! The new version now works on my Bravia Android TV which is great as it\\'s right by my rooftop aerial cable. The scan feature would be useful...any ETA on when this will be available? Also the option to import a list of bookmarks e.g. from a simple properties file would be useful.\"\\nWould you recommend this app to a friend? Not at all, No, Maybe, Yes, or Definitely?', 'Yes']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "imdb None test\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013263940811157227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622d78c0d31a47aa8fac7930d53b5b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 11, original prompts = 10\n",
      "dict_keys(['text', 'label'])\n",
      "number of choices = 2\n",
      "['I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again. The sentiment expressed for the movie is', 'negative']\n",
      "2\n",
      "=====================\n",
      "{{text}} This is definitely not a ||| {{ answer_choices [1-label]}} review.\n",
      "=====================\n",
      "['I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again. This is definitely not a', 'positive review.']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "rotten_tomatoes None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes (../pretrain_models/huggingface/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013541221618652344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4f2d83e27448b39defc0719e9cf58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 10, original prompts = 10\n",
      "dict_keys(['text', 'label'])\n",
      "number of choices = 2\n",
      "['compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children . The sentiment expressed for the movie is', 'positive']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "yelp_review_full None test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_review_full (../pretrain_models/huggingface/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013475418090820312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3577050b6a4145960e80bacb5ac4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 7, original prompts = 7\n",
      "dict_keys(['label', 'text'])\n",
      "number of choices = 5\n",
      "['I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!\\n===\\nBased on that, my rating is', '1 star']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "common_gen None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset common_gen (../pretrain_models/huggingface/common_gen/default/2020.5.30/1a9e8bdc026c41ce7a9e96260debf7d2809cb7fd63fa02b017e4fac1b00c6b23)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013482332229614258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f03df79b9b402fb9de611f2d1c01f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 9, original prompts = 6\n",
      "dict_keys(['concept_set_idx', 'concepts', 'target'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['Humans can easily string together abstract concepts to form a coherent sentence. \\nFor example, with the concepts field, look, stand, a simple sentence can be', 'The player stood in the field looking at the batter.']\n",
      "=====================\n",
      "We have the sentence: {{target}}; \n",
      "Extract all the key concepts: \n",
      "|||\n",
      "{{ concepts | join(\", \") }}\n",
      "=====================\n",
      "['We have the sentence: The player stood in the field looking at the batter.; \\nExtract all the key concepts:', 'field, look, stand']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "wiki_bio None val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (../pretrain_models/huggingface/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013670206069946289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d1881ed4b14b64bc0f1f1c682636a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 5, original prompts = 1\n",
      "dict_keys(['input_text', 'target_text'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[\"Facts:\\n\\n\\n- birth place: egypt\\n\\n\\n\\n- name: michael iii of alexandria\\n\\n\\n\\n\\n\\n- residence: saint mark 's church\\n\\n\\n\\n- feast day: 16 -rrb- march -lrb- 20 baramhat in the coptic calendar\\n\\n\\n\\n- death date: 16 march 907\\n\\n\\n\\n- predecessor: shenouda i\\n\\n\\n\\n- successor: gabriel i\\n\\n\\n\\n- title: 56th of st. mark pope of alexandria & patriarch of the see\\n\\n\\n\\n- religion: coptic orthodox christian\\n\\n\\n\\n- nationality: egyptian\\n\\n\\n\\n- enthroned: 25 april 880\\n\\n\\n\\n- buried: monastery of saint macarius the great\\n\\n\\n\\n- type: pope\\n\\n\\n\\n- ended: 16 march 907\\n\\n\\nBased on these bullet points, write a short biography describing the life of pope michael iii of alexandria\\n.\", 'pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- .\\nin 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .\\nthis building was at one time believed to have later become the site of the cairo geniza .']\n",
      "=====================\n",
      "Read the bio below and try to give details on {{input_text[\"context\"]}}'s: \n",
      "{% for n in range (input_text[\"table\"][\"column_header\"]|length) %} {% if input_text[\"table\"][\"column_header\"][n] != \"article_title\" %}\n",
      "- {{ input_text[\"table\"][\"column_header\"][n].replace(\"_\",\" \") }} \n",
      "{% endif %} {% endfor %}\n",
      "\n",
      "Bio: {{target_text}} |||\n",
      "{% for n in range (input_text[\"table\"][\"column_header\"]|length) %}\n",
      "{% if input_text[\"table\"][\"column_header\"][n] != \"article_title\" %}\n",
      "- {{ input_text[\"table\"][\"column_header\"][n].replace(\"_\",\" \") }} is {{ input_text[\"table\"][\"content\"][n] }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "\n",
      "=====================\n",
      "[\"Read the bio below and try to give details on pope michael iii of alexandria\\n's: \\n \\n- birth place \\n  \\n- name \\n    \\n- residence \\n  \\n- feast day \\n  \\n- death date \\n  \\n- predecessor \\n  \\n- successor \\n  \\n- title \\n  \\n- religion \\n  \\n- nationality \\n  \\n- enthroned \\n  \\n- buried \\n  \\n- type \\n  \\n- ended \\n \\n\\nBio: pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- .\\nin 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .\\nthis building was at one time believed to have later become the site of the cairo geniza .\", \"- birth place is egypt\\n\\n\\n\\n- name is michael iii of alexandria\\n\\n\\n\\n\\n\\n- residence is saint mark 's church\\n\\n\\n\\n- feast day is 16 -rrb- march -lrb- 20 baramhat in the coptic calendar\\n\\n\\n\\n- death date is 16 march 907\\n\\n\\n\\n- predecessor is shenouda i\\n\\n\\n\\n- successor is gabriel i\\n\\n\\n\\n- title is 56th of st. mark pope of alexandria & patriarch of the see\\n\\n\\n\\n- religion is coptic orthodox christian\\n\\n\\n\\n- nationality is egyptian\\n\\n\\n\\n- enthroned is 25 april 880\\n\\n\\n\\n- buried is monastery of saint macarius the great\\n\\n\\n\\n- type is pope\\n\\n\\n\\n- ended is 16 march 907\"]\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "cnn_dailymail 3.0.0 validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (../pretrain_models/huggingface/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013371467590332031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8adf810124e4f0bbe3a7b828c20741c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 9, original prompts = 7\n",
      "dict_keys(['article', 'highlights', 'id'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['In 2 or 3 sentences, what are the main points one should remember from this news article?\\n\\nArticle: (CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don\\'t know, but the fact that so many people can have a life extension, that\\'s pretty big,\" Broussard told CNN affiliate KGO. She may feel guided in her generosity by a higher power. \"Thanks for all the support and prayers,\" a comment on a Facebook page in her name read. \"I know this entire journey is much bigger than all of us. I also know I\\'m just the messenger.\" CNN cannot verify the authenticity of the page. But the power that multiplied Broussard\\'s gift was data processing of genetic profiles from donor-recipient pairs. It works on a simple swapping principle but takes it to a much higher level, according to California Pacific Medical Center in San Francisco. So high, that it is taking five surgeons, a covey of physician assistants, nurses and anesthesiologists, and more than 40 support staff to perform surgeries on 12 people. They are extracting six kidneys from donors and implanting them into six recipients. \"The ages of the donors and recipients range from 26 to 70 and include three parent and child pairs, one sibling pair and one brother and sister-in-law pair,\" the medical center said in a statement. The chain of surgeries is to be wrapped up Friday. In late March, the medical center is planning to hold a reception for all 12 patients. Here\\'s how the super swap works, according to California Pacific Medical Center. Say, your brother needs a kidney to save his life, or at least get off of dialysis, and you\\'re willing to give him one of yours. But then it turns out that your kidney is not a match for him, and it\\'s certain his body would reject it. Your brother can then get on a years-long waiting list for a kidney coming from an organ donor who died. Maybe that...', 'Zully Broussard decided to give a kidney to a stranger .\\nA new computer program helped her donation spur transplants for six kidney patients .']\n",
      "=====================\n",
      "Generate a story from key plot points:\n",
      "\n",
      "{{highlights}} |||\n",
      "{{article}}\n",
      "=====================\n",
      "['Generate a story from key plot points:\\n\\nZully Broussard decided to give a kidney to a stranger .\\nA new computer program helped her donation spur transplants for six kidney patients .', '(CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don\\'t know, but the fact that so many people can have a life extension, that\\'s pretty big,\" Broussard told CNN affiliate KGO. She may feel guided in her generosity by a higher power. \"Thanks for all the support and prayers,\" a comment on a Facebook page in her name read. \"I know this entire journey is much bigger than all of us. I also know I\\'m just the messenger.\" CNN cannot verify the authenticity of the page. But the power that multiplied Broussard\\'s gift was data processing of genetic profiles from donor-recipient pairs. It works on a simple swapping principle but takes it to a much higher level, according to California Pacific Medical Center in San Francisco. So high, that it is taking five surgeons, a covey of physician assistants, nurses and anesthesiologists, and more than 40 support staff to perform surgeries on 12 people. They are extracting six kidneys from donors and implanting them into six recipients. \"The ages of the donors and recipients range from 26 to 70 and include three parent and child pairs, one sibling pair and one brother and sister-in-law pair,\" the medical center said in a statement. The chain of surgeries is to be wrapped up Friday. In late March, the medical center is planning to hold a reception for all 12 patients. Here\\'s how the super swap works, according to California Pacific Medical Center. Say, your brother needs a kidney to save his life, or at least get off of dialysis, and you\\'re willing to give him one of yours. But then it turns out that your kidney is not a match for him, and it\\'s certain his body would reject it. Your brother can then get on a years-long waiting list for a kidney coming from an organ donor who died. Maybe that...']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "gigaword None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset gigaword (../pretrain_models/huggingface/gigaword/default/1.2.0/ea83a8b819190acac5f2dae011fad51dccf269a0604ec5dd24795b64efb424b6)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013448953628540039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ed1309859b40859d3e062ef76145ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 9, original prompts = 7\n",
      "dict_keys(['document', 'summary'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['five-time world champion michelle kwan withdrew from the #### us figure skating championships on wednesday , but will petition us skating officials for the chance to compete at the #### turin olympics .\\n\\nTL;DR:', \"injury leaves kwan 's olympic hopes in limbo\"]\n",
      "=====================\n",
      "Title: {{summary}}\n",
      "\n",
      "||| {{document}}\n",
      "=====================\n",
      "[\"Title: injury leaves kwan 's olympic hopes in limbo\", 'five-time world champion michelle kwan withdrew from the #### us figure skating championships on wednesday , but will petition us skating officials for the chance to compete at the #### turin olympics .']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "multi_news None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset multi_news (../pretrain_models/huggingface/multi_news/default/1.0.0/2f1f69a2bedc8ad1c5d8ae5148e4755ee7095f465c1c01ae8f85454342065a72)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013723134994506836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2e5175465141e59891660b8c2e2f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 6, original prompts = 5\n",
      "dict_keys(['document', 'summary'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['I\\'m trying to distill these articles down into one:\\n\\n\\nArticle: Whether a sign of a good read; or a comment on the \\'pulp\\' nature of some genres of fiction, the Oxfam second-hand book charts have remained in The Da Vinci Code author\\'s favour for the past four years. \\n \\n Dan Brown has topped Oxfam\\'s \\'most donated\\' list again, his fourth consecutive year. Having sold more than 80 million copies of The Da Vinci Code and had all four of his novels on the New York Times bestseller list in the same week, it\\'s hardly surprising that Brown\\'s hefty tomes are being donated to charity by readers keen to make some room on their shelves. \\n \\n Another cult crime writer responsible to heavy-weight hardbacks, Stieg Larsson, is Oxfam\\'s \\'most sold\\' author for the second time in a row. Both the \\'most donated\\' and \\'most sold\\' lists are dominated by crime fiction, trilogies and fantasy, with JK Rowling the only female author listed in either of the Top Fives. \\n \\n Click here or on \"View Gallery\" to see both charts in pictures \\n\\n\\nArticle:  A woman reads a copy of the newly released book \\'\\'The Lost Symbol\\'\\' by Dan Brown, at a speed reading book launch event in Sydney, September 15, 2009. REUTERS/Tim Wimborne \\n \\n SAN FRANCISCO The latest novel from \"Da Vinci Code\" author Dan Brown, \"The Lost Symbol,\" broke one-day sales records, its publisher and booksellers said. \\n \\n Readers snapped up over one million hardcover copies across the United States, Canada and the United Kingdom after it was released on Tuesday, said publisher Knopf Doubleday, a division of Random House Inc. \\n \\n \"We are seeing historic, record-breaking sales across all types of our accounts in North America for \\'The Lost Symbol,\" said Sonny Mehta, editor in chief of Knopf Doubleday Publishing Group. Knopf Doubleday is a division of Random House Inc. \\n \\n Amazon.com Inc, the world\\'s largest online retailer, called the book its bestselling first-day adult fiction title ever, including pre-orders. \\n \\n Barnes & Noble Inc said \"The Lost Symbol\" broke its previous one-day sales record for adult fiction. \\n \\n The success of the Dan Brown\\'s latest is a boost to publisher Knopf Doubleday and booksellers, which have endured sliding sales in the midst of the recession. \\n \\n Booksellers have anxiously awaited a popular title that will resonate with readers and fuel the same sort of frenzy seen earlier this decade with the \"Harry Potter\" series, from author J.K. Rowling. \\n \\n But the $25 billion domestic book market has wallowed in a slump in recent years as more readers make their purchases online, or forego books altogether for online games and other forms of media and entertainment. \\n \\n Now, digital books and electronic reading devices such as Amazon\\'s Kindle or Sony Corp\\'s Reader are seen as both avenues of growth and sources of competition for traditional media, and publishers and booksellers are scrambling to respond. \\n \\n The highly anticipated book from Brown comes six years after the release of the American novelist\\'s last book \"The Da Vinci Code\". It follows the adventures of Harvard...\\n\\n\\nArticle:  Bestselling author is also the most frequently given away to charity shops \\n \\n Dan Brown might be one of the world\\'s bestselling authors but it turns out that readers aren\\'t too keen on keeping his special blend of religious conspiracy and scholarly derring-do on their shelves once they\\'ve bought it. \\n \\n Brown, who has sold more than 81m copies of The Da Vinci Code worldwide, has been revealed as the most donated author to Oxfam\\'s 700 high street shops. With just four books to his name – although his long-awaited fifth The Lost Symbol is published next month – Brown did well to see off competition from John Grisham, author of more than 20 and the second-most likely writer to be ditched in a charity shop by readers. \\n \\n But as secondhand bookshop shelves flood with battered editions of Angels and Demons and Digital Fortress, Brown can comfort himself with the fact that he\\'s also Oxfam\\'s second most bought author: there are, apparently, still readers out there who have yet to follow the adventures of the dapper symbologist Robert Langdon. There\\'s no such consolation for Grisham, whose legal thrillers fail to make Oxfam\\'s bestseller charts at all. \\n \\n \"There\\'s no question that when you go into the back room of Oxfam shops there are many Dan Brown books,\" said Oxfam\\'s director of trading David McCullough. \"But he\\'s also very high on the bestseller list so there is a useful recycling exercise going on – it\\'s not just people saying \\'I\\'ve read The Da Vinci Code and now I must get rid of it\\'.\" \\n \\n Ian Rankin, whose dour, boozy detective John Rebus is no Robert Langdon, tops Oxfam\\'s bestseller list, which the charity says is the first ever high-street secondhand bestseller chart. \"It\\'s always good for an author to know that their books are popular,\" said the Scottish author, who will unveil a new policeman hero, the teetotal Malcolm Fox, next month. \"With Oxfam, it\\'s also heartening to realise that each book donated and bought is helping such a worthwhile organisation.\" \\n \\n Stephenie Meyer, author of the Twilight...\\n\\n\\nArticle:  A charity shop is urging people to stop donating The Da Vinci Code after becoming overwhelmed with copies. \\n \\n The Oxfam shop in Swansea has been receiving an average of one copy of the Dan Brown novel a week for months, leaving them with little room for any other books. \\n \\n Staff who are struggling to sell copies of the book have put a note up in the store saying they would rather donors hand in their vinyl instead.', 'The Da Vinci Code has sold so many copies—that would be at least 80 million—that it\\'s bound to turn up in book donation piles. But at one charity shop in the UK, it\\'s been donated so heavily that the shop has posted a sign propped up on a tower of Da Vinci Code copies that reads: \"You could give us another Da Vinci Code... but we would rather have your vinyl!\" The manager of the Oxfam shop in Swansea tells the Telegraph that people are laughing and taking pictures of the sizable display: \"I would say that we get one copy of the book every day.\" He says people buy them \"occasionally,\" but with vinyl sales up 25% in the past year, they\\'d rather take records. Dan Brown\\'s book isn\\'t the only one that shops like Oxfam struggle to re-sell. Last year, Oxfam was hit with a large and steady supply of Fifty Shades of Grey, and it similarly begged donors: \"Please—no more.\" But Brown has a particular kind of staying power. The Da Vinci Code was published in 2003, and within six years Brown had booted John Grisham from the No. 1 slot on the list of writers whose books were most often donated to Oxfam\\'s 700 shops, reported the Guardian at the time. The Independent in 2012 reported Brown\\'s best-seller was the most-donated book for the fourth year running. (See why Dan Brown took heat from the Philippines.)']\n",
      "=====================\n",
      "{% set docs = document.split(\"3ed2dface8203c4c9dfb1a5dc58e41e0||\") | reject(\"equalto\", \"\") | list%}\n",
      "Write an expanded news article with plausible details from the following summary:\n",
      "{{summary[2:]}}\n",
      "|||\n",
      "{{docs | choice}}\n",
      "=====================\n",
      "['Write an expanded news article with plausible details from the following summary:\\nThe Da Vinci Code has sold so many copies—that would be at least 80 million—that it\\'s bound to turn up in book donation piles. But at one charity shop in the UK, it\\'s been donated so heavily that the shop has posted a sign propped up on a tower of Da Vinci Code copies that reads: \"You could give us another Da Vinci Code... but we would rather have your vinyl!\" The manager of the Oxfam shop in Swansea tells the Telegraph that people are laughing and taking pictures of the sizable display: \"I would say that we get one copy of the book every day.\" He says people buy them \"occasionally,\" but with vinyl sales up 25% in the past year, they\\'d rather take records. Dan Brown\\'s book isn\\'t the only one that shops like Oxfam struggle to re-sell. Last year, Oxfam was hit with a large and steady supply of Fifty Shades of Grey, and it similarly begged donors: \"Please—no more.\" But Brown has a particular kind of staying power. The Da Vinci Code was published in 2003, and within six years Brown had booted John Grisham from the No. 1 slot on the list of writers whose books were most often donated to Oxfam\\'s 700 shops, reported the Guardian at the time. The Independent in 2012 reported Brown\\'s best-seller was the most-donated book for the fourth year running. (See why Dan Brown took heat from the Philippines.)', 'A woman reads a copy of the newly released book \\'\\'The Lost Symbol\\'\\' by Dan Brown, at a speed reading book launch event in Sydney, September 15, 2009. REUTERS/Tim Wimborne \\n \\n SAN FRANCISCO The latest novel from \"Da Vinci Code\" author Dan Brown, \"The Lost Symbol,\" broke one-day sales records, its publisher and booksellers said. \\n \\n Readers snapped up over one million hardcover copies across the United States, Canada and the United Kingdom after it was released on Tuesday, said publisher Knopf Doubleday, a division of Random House Inc. \\n \\n \"We are seeing historic, record-breaking sales across all types of our accounts in North America for \\'The Lost Symbol,\" said Sonny Mehta, editor in chief of Knopf Doubleday Publishing Group. Knopf Doubleday is a division of Random House Inc. \\n \\n Amazon.com Inc, the world\\'s largest online retailer, called the book its bestselling first-day adult fiction title ever, including pre-orders. \\n \\n Barnes & Noble Inc said \"The Lost Symbol\" broke its previous one-day sales record for adult fiction. \\n \\n The success of the Dan Brown\\'s latest is a boost to publisher Knopf Doubleday and booksellers, which have endured sliding sales in the midst of the recession. \\n \\n Booksellers have anxiously awaited a popular title that will resonate with readers and fuel the same sort of frenzy seen earlier this decade with the \"Harry Potter\" series, from author J.K. Rowling. \\n \\n But the $25 billion domestic book market has wallowed in a slump in recent years as more readers make their purchases online, or forego books altogether for online games and other forms of media and entertainment. \\n \\n Now, digital books and electronic reading devices such as Amazon\\'s Kindle or Sony Corp\\'s Reader are seen as both avenues of growth and sources of competition for traditional media, and publishers and booksellers are scrambling to respond. \\n \\n The highly anticipated book from Brown comes six years after the release of the American novelist\\'s last book \"The Da Vinci Code\". It follows the adventures of Harvard...']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "xsum None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (../pretrain_models/huggingface/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013717889785766602,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356a6390efc94ac09c903a282f0d8c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 10, original prompts = 10\n",
      "dict_keys(['document', 'summary', 'id'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "['The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - a charity to raise money for Nigerian sport.\\nMr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\\nAppearing at the Old Bailey earlier, all four denied the offence.\\nThe charge relates to offences which allegedly took place between 2008 and 2014.\\nSam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand trial in July.\\nThey were all released on bail.\\nThis boils down to the simple idea that', 'Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of charity fraud.']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "samsum None validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset samsum (../pretrain_models/huggingface/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013870477676391602,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0889f2c88a408b9414d25f1cd47b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 7, original prompts = 6\n",
      "dict_keys(['id', 'dialogue', 'summary'])\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "[\"Generate a summary for this dialogue:\\nA: Hi Tom, are you busy tomorrow’s afternoon?\\r\\nB: I’m pretty sure I am. What’s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\r\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\", 'A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.']\n",
      "=====================\n",
      "Write a dialogue that matches this summary: {{summary}} |||\n",
      "{{dialogue}}\n",
      "=====================\n",
      "['Write a dialogue that matches this summary: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy.', \"A: Hi Tom, are you busy tomorrow’s afternoon?\\r\\nB: I’m pretty sure I am. What’s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\r\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\"]\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "ag_news None test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (../pretrain_models/huggingface/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013972043991088867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fb632bc01745cbb92d12f69ba84c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 7, original prompts = 7\n",
      "dict_keys(['text', 'label'])\n",
      "number of choices = 4\n",
      "[\"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul. \\nWhat label best describes this news article?\", 'Business']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "dbpedia_14 None test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset dbpedia_14 (../pretrain_models/huggingface/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013618946075439453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a48dc6a5957474dbf0ba72650698ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 4, original prompts = 4\n",
      "dict_keys(['label', 'title', 'content'])\n",
      "number of choices = 14\n",
      "[\"TY KU -  TY KU /taɪkuː/ is an American alcoholic beverage company that specializes in sake and other spirits. The privately-held company was founded in 2004 and is headquartered in New York City New York. While based in New York TY KU's beverages are made in Japan through a joint venture with two sake breweries. Since 2011 TY KU's growth has extended its products into all 50 states. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\", 'Company']\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "trec None test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset trec (../pretrain_models/huggingface/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014028549194335938,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101a80e9da1946e19f3b0475f1f959b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompts = 18, original prompts = 7\n",
      "dict_keys(['label-coarse', 'label-fine', 'text'])\n",
      "number of choices = 2\n",
      "['What is this question asking for?\\n\\nHow far is it from Denver to Aspen ?', '']\n",
      "2\n",
      "=====================\n",
      "{% set label_mapping={34:0, 3:1} %} \n",
      "{% if label_coarse == 2 %}\n",
      "Is this question asking for an {{', '.join(answer_choices)}}?\n",
      "{{text}}\n",
      "|||\n",
      "{{answer_choices[label_mapping[label_fine]] }}\n",
      "{% endif %}\n",
      "=====================\n",
      "['']\n",
      "datasets with more than 2 original prompts: 47\n",
      "datasets with only non-original prompts: 3\n"
     ]
    }
   ],
   "source": [
    "! export TRANSFORMERS_CACHE=../pretrain_models/huggingface\n",
    "! export HF_DATASETS_CACHE=../pretrain_models/huggingface\n",
    "! export HF_METRICS_CACHE=../pretrain_models/huggingface\n",
    "! cache_dir=../pretrain_models/huggingface\n",
    "\n",
    "# T0 train set\n",
    "\n",
    "from os import read\n",
    "from promptsource.templates import DatasetTemplates\n",
    "import datasets\n",
    "cache_dir='../pretrain_models/huggingface'\n",
    "\n",
    "\n",
    "datasets_with_more_than_2_original_prompts = 0\n",
    "datasets_with_only_nonoriginal_prompts = 0\n",
    "\n",
    "def read_prompts(dataset, subset, split):\n",
    "    global datasets_with_more_than_2_original_prompts\n",
    "    global datasets_with_only_nonoriginal_prompts\n",
    "    print()\n",
    "    print(\"===\"*100)\n",
    "    print(dataset, subset, split)\n",
    "    data = datasets.load_dataset(dataset, subset, cache_dir=cache_dir)\n",
    "    data = data[split]\n",
    "    prompts = DatasetTemplates(dataset, subset)\n",
    "    prompt_names = prompts.all_template_names\n",
    "    original_prompts =[n for n in prompt_names if prompts[n].metadata.original_task]\n",
    "    non_original_prompts = [n for n in prompt_names if not prompts[n].metadata.original_task]\n",
    "    print(\"total prompts = {}, original prompts = {}\".format(len(prompt_names), len(original_prompts)))\n",
    "\n",
    "    choices = prompts[prompt_names[0]].get_answer_choices_list(data[0])\n",
    "    print(data[0].keys())\n",
    "    if choices is not None:\n",
    "        print(\"number of choices = {}\".format(len(choices)))\n",
    "    else:\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>> no choices >>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    if len(original_prompts) > 0:\n",
    "        print(prompts[original_prompts[0]].apply(data[0]))\n",
    "    if len(non_original_prompts) > 0:\n",
    "        cc = prompts[non_original_prompts[0]].get_answer_choices_list(data[0])\n",
    "        if cc is not None:\n",
    "            print(len(cc))\n",
    "        print(\"=====================\")\n",
    "        print(prompts[non_original_prompts[0]].jinja)\n",
    "        print(\"=====================\")\n",
    "        print(prompts[non_original_prompts[0]].apply(data[0]))\n",
    "    \n",
    "    if len(original_prompts) >= 2:\n",
    "        datasets_with_more_than_2_original_prompts += 1\n",
    "    if len(original_prompts) == 0:\n",
    "        datasets_with_only_nonoriginal_prompts += 1\n",
    "\n",
    "# Paraphrase Identification glue/mrpc\n",
    "read_prompts(\"glue\", \"mrpc\", \"validation\")\n",
    "# Paraphrase Identification glue/qqp\n",
    "read_prompts(\"glue\", 'qqp', \"validation\")\n",
    "# Paraphrase Identification paws/labeled_final\n",
    "read_prompts(\"paws\", 'labeled_final', \"validation\")\n",
    "\n",
    "# Closed-Book QA ai2_arc/ARC_Challenge\n",
    "read_prompts(\"ai2_arc\", 'ARC-Challenge', \"validation\")\n",
    "# Closed-Book QA ai2_arc/ARC_Easy\n",
    "read_prompts(\"ai2_arc\", 'ARC-Easy', \"validation\")\n",
    "# Closed-Book QA kilt_tasks/hotpotqa\n",
    "read_prompts(\"kilt_tasks\", 'hotpotqa', \"validation\")\n",
    "# Closed-Book QA trivia_qa/unfiltered\n",
    "read_prompts(\"trivia_qa\", 'unfiltered', \"validation\")\n",
    "# Closed-Book QA web_questions\n",
    "read_prompts(\"web_questions\", None, \"test\")\n",
    "# Closed-Book QA wiki_qa\n",
    "read_prompts(\"wiki_qa\", None, \"validation\")\n",
    "\n",
    "# Extractive QA adversarial_qa/dbidaf\n",
    "read_prompts(\"adversarial_qa\", 'dbidaf', \"validation\")\n",
    "# Extractive QA adversarial_qa/dbert\n",
    "read_prompts(\"adversarial_qa\", 'dbert', \"validation\")\n",
    "# Extractive QA adversarial_qa/droberta\n",
    "read_prompts(\"adversarial_qa\", 'droberta', \"validation\")\n",
    "# Extractive QA duorc/SelfRC\n",
    "read_prompts(\"duorc\", 'SelfRC', \"validation\")\n",
    "# Extractive QA duorc/ParaphraseRC\n",
    "read_prompts(\"duorc\", 'ParaphraseRC', \"validation\")\n",
    "# Extractive QA ropes\n",
    "read_prompts(\"ropes\", None, \"validation\")\n",
    "# Extractive QA squad_v2\n",
    "read_prompts(\"squad_v2\", None, \"validation\")\n",
    "# Extractive QA super_glue/record\n",
    "read_prompts(\"super_glue\", 'record', \"validation\")\n",
    "# Extractive QA quoref\n",
    "read_prompts(\"quoref\", None, \"validation\")\n",
    "# Extractive QA tydiqa\n",
    "read_prompts(\"tydiqa\", \"primary_task\", \"validation\")\n",
    "\n",
    "# Multiple-Choice QA cos_e/v1.11\n",
    "read_prompts(\"cos_e\", 'v1.11', \"validation\")\n",
    "# Multiple-Choice QA cosmos_qa\n",
    "read_prompts(\"cosmos_qa\", None, \"validation\")\n",
    "# Multiple-Choice QA dream\n",
    "read_prompts(\"dream\", None, \"validation\")\n",
    "# Multiple-Choice QA openbookqa/main \n",
    "read_prompts(\"openbookqa\", 'main', \"validation\")\n",
    "# Multiple-Choice QA qasc\n",
    "read_prompts(\"qasc\", None, \"validation\")\n",
    "# Multiple-Choice QA quail \n",
    "read_prompts(\"quail\", None, \"validation\")\n",
    "# Multiple-Choice QA quarel \n",
    "read_prompts(\"quarel\", None, \"validation\")\n",
    "# Multiple-Choice QA quartz\n",
    "read_prompts(\"quartz\", None, \"validation\")\n",
    "# Multiple-Choice QA race/high\n",
    "read_prompts(\"race\", \"high\", \"validation\")\n",
    "# Multiple-Choice QA race/middle\n",
    "read_prompts(\"race\", \"middle\", \"validation\")\n",
    "# Multiple-Choice QA sciq\n",
    "read_prompts(\"sciq\", None, \"validation\")\n",
    "# Multiple-Choice QA social_i_qa\n",
    "read_prompts(\"social_i_qa\", None, \"validation\")\n",
    "# Multiple-Choice QA super_glue/boolq\n",
    "read_prompts(\"super_glue\", 'boolq', \"validation\")\n",
    "# Multiple-Choice QA super_glue/multirc\n",
    "read_prompts(\"super_glue\", 'multirc', \"validation\")\n",
    "# Multiple-Choice QA wiki_hop/original\n",
    "read_prompts(\"wiki_hop\", 'original', \"validation\")\n",
    "# Multiple-Choice QA wiqa \n",
    "read_prompts(\"wiqa\", None, \"validation\")\n",
    "# Multiple-Choice QA piqa\n",
    "read_prompts(\"piqa\", None, \"validation\")\n",
    "\n",
    "# Sentiment amazon_polarity\n",
    "read_prompts(\"amazon_polarity\", None, \"test\")\n",
    "# Sentiment app_reviews\n",
    "read_prompts(\"app_reviews\", None, \"train\")\n",
    "# Sentiment imdb\n",
    "read_prompts(\"imdb\", None, \"test\")\n",
    "# Sentiment rotten_tomatoes\n",
    "read_prompts(\"rotten_tomatoes\", None, \"validation\")\n",
    "# Sentiment yelp_review_full\n",
    "read_prompts(\"yelp_review_full\", None, \"test\")\n",
    "\n",
    "# Structure-to-Text common_gen\n",
    "read_prompts(\"common_gen\", None, \"validation\")\n",
    "# Structure-to-Text wiki_bio\n",
    "read_prompts(\"wiki_bio\", None, \"val\")\n",
    "\n",
    "# Summarization cnn_dailymail/3.0.0\n",
    "read_prompts(\"cnn_dailymail\", '3.0.0', \"validation\")\n",
    "# Summarization gigaword\n",
    "read_prompts(\"gigaword\", None, \"validation\")\n",
    "# Summarization multi_news\n",
    "read_prompts(\"multi_news\", None, \"validation\")\n",
    "# Summarization xsum\n",
    "read_prompts(\"xsum\", None, \"validation\")\n",
    "# Summarization samsum\n",
    "read_prompts(\"samsum\", None, \"validation\")\n",
    "\n",
    "# Topic Classification ag_news\n",
    "read_prompts(\"ag_news\", None, \"test\")\n",
    "# Topic Classification dbpedia_14\n",
    "read_prompts(\"dbpedia_14\", None, \"test\")\n",
    "# Topic Classification trec\n",
    "read_prompts(\"trec\", None, \"test\")\n",
    "\n",
    "print(\"datasets with more than 2 original prompts: {}\".format(datasets_with_more_than_2_original_prompts))\n",
    "print(\"datasets with only non-original prompts: {}\".format(datasets_with_only_nonoriginal_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b2fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 18:50:27.308876: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-19 18:50:31.851941: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "To be able to use bigscience/P3, you need to install the following dependency: tensorflow.\nPlease install it using 'pip install tensorflow' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(prompts[prompt_names[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(data[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# read_prompts(\"hotpot_qa\", \"fullwiki\", \"validation\")\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigscience/P3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/t0/lib/python3.8/site-packages/datasets/load.py:1723\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[1;32m   1722\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1723\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1724\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1725\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1726\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1727\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1728\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1729\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1730\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1731\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1732\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1733\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1734\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1735\u001b[0m )\n\u001b[1;32m   1737\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda/envs/t0/lib/python3.8/site-packages/datasets/load.py:1500\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1499\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1500\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1501\u001b[0m     path,\n\u001b[1;32m   1502\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1503\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1504\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1505\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1506\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1507\u001b[0m )\n\u001b[1;32m   1509\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/miniconda/envs/t0/lib/python3.8/site-packages/datasets/load.py:1247\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1243\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1244\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m-> 1247\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1248\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1251\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/t0/lib/python3.8/site-packages/datasets/load.py:1220\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1219\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39min\u001b[39;00m [sibling\u001b[39m.\u001b[39mrfilename \u001b[39mfor\u001b[39;00m sibling \u001b[39min\u001b[39;00m dataset_info\u001b[39m.\u001b[39msiblings]:\n\u001b[0;32m-> 1220\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1221\u001b[0m         path,\n\u001b[1;32m   1222\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1223\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1224\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1225\u001b[0m         dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[1;32m   1226\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1229\u001b[0m         path,\n\u001b[1;32m   1230\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[1;32m   1235\u001b[0m     )\u001b[39m.\u001b[39mget_module()\n",
      "File \u001b[0;32m~/miniconda/envs/t0/lib/python3.8/site-packages/datasets/load.py:931\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m dataset_infos_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_dataset_infos_file()\n\u001b[1;32m    930\u001b[0m imports \u001b[39m=\u001b[39m get_imports(local_path)\n\u001b[0;32m--> 931\u001b[0m local_imports \u001b[39m=\u001b[39m _download_additional_modules(\n\u001b[1;32m    932\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    933\u001b[0m     base_path\u001b[39m=\u001b[39;49mhf_hub_url(repo_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, revision\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrevision),\n\u001b[1;32m    934\u001b[0m     imports\u001b[39m=\u001b[39;49mimports,\n\u001b[1;32m    935\u001b[0m     download_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_config,\n\u001b[1;32m    936\u001b[0m )\n\u001b[1;32m    937\u001b[0m additional_files \u001b[39m=\u001b[39m [(config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME, dataset_infos_path)] \u001b[39mif\u001b[39;00m dataset_infos_path \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    938\u001b[0m \u001b[39m# copy the script and the files in an importable directory\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/t0/lib/python3.8/site-packages/datasets/load.py:215\u001b[0m, in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    213\u001b[0m     _depencencies_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdependencies\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(needs_to_be_installed) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdependency\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m     _them_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mthem\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(needs_to_be_installed) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mit\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    216\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTo be able to use \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m, you need to install the following \u001b[39m\u001b[39m{\u001b[39;00m_depencencies_str\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(needs_to_be_installed)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPlease install \u001b[39m\u001b[39m{\u001b[39;00m_them_str\u001b[39m}\u001b[39;00m\u001b[39m using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpip install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(needs_to_be_installed\u001b[39m.\u001b[39mvalues())\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for instance\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m local_imports\n",
      "\u001b[0;31mImportError\u001b[0m: To be able to use bigscience/P3, you need to install the following dependency: tensorflow.\nPlease install it using 'pip install tensorflow' for instance'"
     ]
    }
   ],
   "source": [
    "! export TRANSFORMERS_CACHE=../pretrain_models/huggingface\n",
    "! export HF_DATASETS_CACHE=../pretrain_models/huggingface\n",
    "! export HF_METRICS_CACHE=../pretrain_models/huggingface\n",
    "! cache_dir=../pretrain_models/huggingface\n",
    "\n",
    "# T0 train set\n",
    "\n",
    "from os import read\n",
    "from promptsource.templates import DatasetTemplates\n",
    "import datasets\n",
    "cache_dir='../pretrain_models/huggingface'\n",
    "\n",
    "def read_prompts(dataset, subset, split):\n",
    "    print()\n",
    "    print(\"===\"*100)\n",
    "    print(dataset, subset, split)\n",
    "    data = datasets.load_dataset(dataset, subset, cache_dir=cache_dir)\n",
    "    data = data[split]\n",
    "    prompts = DatasetTemplates(dataset, subset)\n",
    "    prompt_names = prompts.all_template_names\n",
    "    print(len(prompt_names))\n",
    "\n",
    "    choices = prompts[prompt_names[0]].get_answer_choices_list(data[0])\n",
    "    if choices is not None:\n",
    "        print(data[0].keys())\n",
    "        print(prompts[prompt_names[0]].jinja)\n",
    "        print(\"number of choices = {}\".format(len(choices)))\n",
    "    else:\n",
    "        print(\"======= no choices =========\")\n",
    "        print(data[0].keys())\n",
    "        print(prompts[prompt_names[0]].jinja)\n",
    "        print(prompts[prompt_names[0]].apply(data[0]))\n",
    "       \n",
    "# read_prompts(\"hotpot_qa\", \"fullwiki\", \"validation\")\n",
    "\n",
    "data = datasets.load_dataset('bigscience/P3', cache_dir=cache_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('t0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "428134b7041272f7f6fb8cf407c44ed49b4dcd39ed36c80b59bf47114dcbc330"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
